{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Q440bZK4gjwxikDrHc1XBiEhiAwH6wPG",
      "authorship_tag": "ABX9TyN6/yiJqBf+wbFZsGkqPqqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swimskykim00/SW-project/blob/main/%EA%B0%9C%EC%9D%B8%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B82(Mvtec).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ICb2SgxgfHy",
        "outputId": "ef41becf-55f8-463c-da95-7d9b59e1f49e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "!xz -d /content/drive/MyDrive/SW사업/carpet.tar.xz"
      ],
      "metadata": {
        "id": "vebyxWxUitHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4IR4JKCEiquM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvf /content/drive/MyDrive/SW사업/carpet.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6lTpllFiSCa",
        "outputId": "d39a74ed-1bf5-4dd5-be93-5dae15dc9f58"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "carpet/\n",
            "carpet/ground_truth/\n",
            "carpet/ground_truth/color/\n",
            "carpet/ground_truth/color/016_mask.png\n",
            "carpet/ground_truth/color/017_mask.png\n",
            "carpet/ground_truth/color/018_mask.png\n",
            "carpet/ground_truth/color/000_mask.png\n",
            "carpet/ground_truth/color/001_mask.png\n",
            "carpet/ground_truth/color/002_mask.png\n",
            "carpet/ground_truth/color/003_mask.png\n",
            "carpet/ground_truth/color/004_mask.png\n",
            "carpet/ground_truth/color/005_mask.png\n",
            "carpet/ground_truth/color/006_mask.png\n",
            "carpet/ground_truth/color/007_mask.png\n",
            "carpet/ground_truth/color/008_mask.png\n",
            "carpet/ground_truth/color/009_mask.png\n",
            "carpet/ground_truth/color/010_mask.png\n",
            "carpet/ground_truth/color/011_mask.png\n",
            "carpet/ground_truth/color/012_mask.png\n",
            "carpet/ground_truth/color/013_mask.png\n",
            "carpet/ground_truth/color/014_mask.png\n",
            "carpet/ground_truth/color/015_mask.png\n",
            "carpet/ground_truth/cut/\n",
            "carpet/ground_truth/cut/000_mask.png\n",
            "carpet/ground_truth/cut/001_mask.png\n",
            "carpet/ground_truth/cut/002_mask.png\n",
            "carpet/ground_truth/cut/003_mask.png\n",
            "carpet/ground_truth/cut/004_mask.png\n",
            "carpet/ground_truth/cut/005_mask.png\n",
            "carpet/ground_truth/cut/006_mask.png\n",
            "carpet/ground_truth/cut/007_mask.png\n",
            "carpet/ground_truth/cut/008_mask.png\n",
            "carpet/ground_truth/cut/009_mask.png\n",
            "carpet/ground_truth/cut/010_mask.png\n",
            "carpet/ground_truth/cut/011_mask.png\n",
            "carpet/ground_truth/cut/012_mask.png\n",
            "carpet/ground_truth/cut/013_mask.png\n",
            "carpet/ground_truth/cut/014_mask.png\n",
            "carpet/ground_truth/cut/015_mask.png\n",
            "carpet/ground_truth/cut/016_mask.png\n",
            "carpet/ground_truth/hole/\n",
            "carpet/ground_truth/hole/000_mask.png\n",
            "carpet/ground_truth/hole/001_mask.png\n",
            "carpet/ground_truth/hole/002_mask.png\n",
            "carpet/ground_truth/hole/003_mask.png\n",
            "carpet/ground_truth/hole/004_mask.png\n",
            "carpet/ground_truth/hole/005_mask.png\n",
            "carpet/ground_truth/hole/006_mask.png\n",
            "carpet/ground_truth/hole/007_mask.png\n",
            "carpet/ground_truth/hole/008_mask.png\n",
            "carpet/ground_truth/hole/009_mask.png\n",
            "carpet/ground_truth/hole/010_mask.png\n",
            "carpet/ground_truth/hole/011_mask.png\n",
            "carpet/ground_truth/hole/012_mask.png\n",
            "carpet/ground_truth/hole/013_mask.png\n",
            "carpet/ground_truth/hole/014_mask.png\n",
            "carpet/ground_truth/hole/015_mask.png\n",
            "carpet/ground_truth/hole/016_mask.png\n",
            "carpet/ground_truth/metal_contamination/\n",
            "carpet/ground_truth/metal_contamination/000_mask.png\n",
            "carpet/ground_truth/metal_contamination/001_mask.png\n",
            "carpet/ground_truth/metal_contamination/002_mask.png\n",
            "carpet/ground_truth/metal_contamination/003_mask.png\n",
            "carpet/ground_truth/metal_contamination/004_mask.png\n",
            "carpet/ground_truth/metal_contamination/005_mask.png\n",
            "carpet/ground_truth/metal_contamination/006_mask.png\n",
            "carpet/ground_truth/metal_contamination/007_mask.png\n",
            "carpet/ground_truth/metal_contamination/008_mask.png\n",
            "carpet/ground_truth/metal_contamination/009_mask.png\n",
            "carpet/ground_truth/metal_contamination/010_mask.png\n",
            "carpet/ground_truth/metal_contamination/011_mask.png\n",
            "carpet/ground_truth/metal_contamination/012_mask.png\n",
            "carpet/ground_truth/metal_contamination/013_mask.png\n",
            "carpet/ground_truth/metal_contamination/014_mask.png\n",
            "carpet/ground_truth/metal_contamination/015_mask.png\n",
            "carpet/ground_truth/metal_contamination/016_mask.png\n",
            "carpet/ground_truth/thread/\n",
            "carpet/ground_truth/thread/018_mask.png\n",
            "carpet/ground_truth/thread/000_mask.png\n",
            "carpet/ground_truth/thread/001_mask.png\n",
            "carpet/ground_truth/thread/002_mask.png\n",
            "carpet/ground_truth/thread/003_mask.png\n",
            "carpet/ground_truth/thread/004_mask.png\n",
            "carpet/ground_truth/thread/005_mask.png\n",
            "carpet/ground_truth/thread/006_mask.png\n",
            "carpet/ground_truth/thread/007_mask.png\n",
            "carpet/ground_truth/thread/008_mask.png\n",
            "carpet/ground_truth/thread/009_mask.png\n",
            "carpet/ground_truth/thread/010_mask.png\n",
            "carpet/ground_truth/thread/011_mask.png\n",
            "carpet/ground_truth/thread/012_mask.png\n",
            "carpet/ground_truth/thread/013_mask.png\n",
            "carpet/ground_truth/thread/014_mask.png\n",
            "carpet/ground_truth/thread/015_mask.png\n",
            "carpet/ground_truth/thread/016_mask.png\n",
            "carpet/ground_truth/thread/017_mask.png\n",
            "carpet/test/\n",
            "carpet/test/color/\n",
            "carpet/test/color/000.png\n",
            "carpet/test/color/001.png\n",
            "carpet/test/color/002.png\n",
            "carpet/test/color/003.png\n",
            "carpet/test/color/004.png\n",
            "carpet/test/color/005.png\n",
            "carpet/test/color/006.png\n",
            "carpet/test/color/007.png\n",
            "carpet/test/color/008.png\n",
            "carpet/test/color/009.png\n",
            "carpet/test/color/010.png\n",
            "carpet/test/color/011.png\n",
            "carpet/test/color/012.png\n",
            "carpet/test/color/013.png\n",
            "carpet/test/color/014.png\n",
            "carpet/test/color/015.png\n",
            "carpet/test/color/016.png\n",
            "carpet/test/color/017.png\n",
            "carpet/test/color/018.png\n",
            "carpet/test/cut/\n",
            "carpet/test/cut/000.png\n",
            "carpet/test/cut/001.png\n",
            "carpet/test/cut/002.png\n",
            "carpet/test/cut/003.png\n",
            "carpet/test/cut/004.png\n",
            "carpet/test/cut/005.png\n",
            "carpet/test/cut/006.png\n",
            "carpet/test/cut/007.png\n",
            "carpet/test/cut/008.png\n",
            "carpet/test/cut/009.png\n",
            "carpet/test/cut/010.png\n",
            "carpet/test/cut/011.png\n",
            "carpet/test/cut/012.png\n",
            "carpet/test/cut/013.png\n",
            "carpet/test/cut/014.png\n",
            "carpet/test/cut/015.png\n",
            "carpet/test/cut/016.png\n",
            "carpet/test/good/\n",
            "carpet/test/good/025.png\n",
            "carpet/test/good/026.png\n",
            "carpet/test/good/027.png\n",
            "carpet/test/good/000.png\n",
            "carpet/test/good/001.png\n",
            "carpet/test/good/002.png\n",
            "carpet/test/good/003.png\n",
            "carpet/test/good/004.png\n",
            "carpet/test/good/005.png\n",
            "carpet/test/good/006.png\n",
            "carpet/test/good/007.png\n",
            "carpet/test/good/008.png\n",
            "carpet/test/good/009.png\n",
            "carpet/test/good/010.png\n",
            "carpet/test/good/011.png\n",
            "carpet/test/good/012.png\n",
            "carpet/test/good/013.png\n",
            "carpet/test/good/014.png\n",
            "carpet/test/good/015.png\n",
            "carpet/test/good/016.png\n",
            "carpet/test/good/017.png\n",
            "carpet/test/good/018.png\n",
            "carpet/test/good/019.png\n",
            "carpet/test/good/020.png\n",
            "carpet/test/good/021.png\n",
            "carpet/test/good/022.png\n",
            "carpet/test/good/023.png\n",
            "carpet/test/good/024.png\n",
            "carpet/test/hole/\n",
            "carpet/test/hole/000.png\n",
            "carpet/test/hole/001.png\n",
            "carpet/test/hole/002.png\n",
            "carpet/test/hole/003.png\n",
            "carpet/test/hole/004.png\n",
            "carpet/test/hole/005.png\n",
            "carpet/test/hole/006.png\n",
            "carpet/test/hole/007.png\n",
            "carpet/test/hole/008.png\n",
            "carpet/test/hole/009.png\n",
            "carpet/test/hole/010.png\n",
            "carpet/test/hole/011.png\n",
            "carpet/test/hole/012.png\n",
            "carpet/test/hole/013.png\n",
            "carpet/test/hole/014.png\n",
            "carpet/test/hole/015.png\n",
            "carpet/test/hole/016.png\n",
            "carpet/test/metal_contamination/\n",
            "carpet/test/metal_contamination/000.png\n",
            "carpet/test/metal_contamination/001.png\n",
            "carpet/test/metal_contamination/002.png\n",
            "carpet/test/metal_contamination/003.png\n",
            "carpet/test/metal_contamination/004.png\n",
            "carpet/test/metal_contamination/005.png\n",
            "carpet/test/metal_contamination/006.png\n",
            "carpet/test/metal_contamination/007.png\n",
            "carpet/test/metal_contamination/008.png\n",
            "carpet/test/metal_contamination/009.png\n",
            "carpet/test/metal_contamination/010.png\n",
            "carpet/test/metal_contamination/011.png\n",
            "carpet/test/metal_contamination/012.png\n",
            "carpet/test/metal_contamination/013.png\n",
            "carpet/test/metal_contamination/014.png\n",
            "carpet/test/metal_contamination/015.png\n",
            "carpet/test/metal_contamination/016.png\n",
            "carpet/test/thread/\n",
            "carpet/test/thread/000.png\n",
            "carpet/test/thread/001.png\n",
            "carpet/test/thread/002.png\n",
            "carpet/test/thread/003.png\n",
            "carpet/test/thread/004.png\n",
            "carpet/test/thread/005.png\n",
            "carpet/test/thread/006.png\n",
            "carpet/test/thread/007.png\n",
            "carpet/test/thread/008.png\n",
            "carpet/test/thread/009.png\n",
            "carpet/test/thread/010.png\n",
            "carpet/test/thread/011.png\n",
            "carpet/test/thread/012.png\n",
            "carpet/test/thread/013.png\n",
            "carpet/test/thread/014.png\n",
            "carpet/test/thread/015.png\n",
            "carpet/test/thread/016.png\n",
            "carpet/test/thread/017.png\n",
            "carpet/test/thread/018.png\n",
            "carpet/train/\n",
            "carpet/train/good/\n",
            "carpet/train/good/000.png\n",
            "carpet/train/good/001.png\n",
            "carpet/train/good/002.png\n",
            "carpet/train/good/003.png\n",
            "carpet/train/good/004.png\n",
            "carpet/train/good/005.png\n",
            "carpet/train/good/006.png\n",
            "carpet/train/good/007.png\n",
            "carpet/train/good/008.png\n",
            "carpet/train/good/009.png\n",
            "carpet/train/good/010.png\n",
            "carpet/train/good/011.png\n",
            "carpet/train/good/012.png\n",
            "carpet/train/good/013.png\n",
            "carpet/train/good/014.png\n",
            "carpet/train/good/015.png\n",
            "carpet/train/good/016.png\n",
            "carpet/train/good/017.png\n",
            "carpet/train/good/018.png\n",
            "carpet/train/good/019.png\n",
            "carpet/train/good/020.png\n",
            "carpet/train/good/021.png\n",
            "carpet/train/good/022.png\n",
            "carpet/train/good/023.png\n",
            "carpet/train/good/024.png\n",
            "carpet/train/good/025.png\n",
            "carpet/train/good/026.png\n",
            "carpet/train/good/027.png\n",
            "carpet/train/good/028.png\n",
            "carpet/train/good/029.png\n",
            "carpet/train/good/030.png\n",
            "carpet/train/good/130.png\n",
            "carpet/train/good/131.png\n",
            "carpet/train/good/132.png\n",
            "carpet/train/good/133.png\n",
            "carpet/train/good/134.png\n",
            "carpet/train/good/135.png\n",
            "carpet/train/good/136.png\n",
            "carpet/train/good/137.png\n",
            "carpet/train/good/138.png\n",
            "carpet/train/good/139.png\n",
            "carpet/train/good/140.png\n",
            "carpet/train/good/141.png\n",
            "carpet/train/good/142.png\n",
            "carpet/train/good/143.png\n",
            "carpet/train/good/144.png\n",
            "carpet/train/good/145.png\n",
            "carpet/train/good/146.png\n",
            "carpet/train/good/147.png\n",
            "carpet/train/good/148.png\n",
            "carpet/train/good/149.png\n",
            "carpet/train/good/150.png\n",
            "carpet/train/good/151.png\n",
            "carpet/train/good/152.png\n",
            "carpet/train/good/153.png\n",
            "carpet/train/good/154.png\n",
            "carpet/train/good/155.png\n",
            "carpet/train/good/156.png\n",
            "carpet/train/good/157.png\n",
            "carpet/train/good/158.png\n",
            "carpet/train/good/159.png\n",
            "carpet/train/good/160.png\n",
            "carpet/train/good/161.png\n",
            "carpet/train/good/162.png\n",
            "carpet/train/good/163.png\n",
            "carpet/train/good/164.png\n",
            "carpet/train/good/165.png\n",
            "carpet/train/good/166.png\n",
            "carpet/train/good/167.png\n",
            "carpet/train/good/168.png\n",
            "carpet/train/good/169.png\n",
            "carpet/train/good/170.png\n",
            "carpet/train/good/171.png\n",
            "carpet/train/good/172.png\n",
            "carpet/train/good/173.png\n",
            "carpet/train/good/174.png\n",
            "carpet/train/good/175.png\n",
            "carpet/train/good/176.png\n",
            "carpet/train/good/177.png\n",
            "carpet/train/good/178.png\n",
            "carpet/train/good/179.png\n",
            "carpet/train/good/180.png\n",
            "carpet/train/good/181.png\n",
            "carpet/train/good/182.png\n",
            "carpet/train/good/183.png\n",
            "carpet/train/good/184.png\n",
            "carpet/train/good/185.png\n",
            "carpet/train/good/186.png\n",
            "carpet/train/good/187.png\n",
            "carpet/train/good/188.png\n",
            "carpet/train/good/189.png\n",
            "carpet/train/good/190.png\n",
            "carpet/train/good/191.png\n",
            "carpet/train/good/192.png\n",
            "carpet/train/good/193.png\n",
            "carpet/train/good/194.png\n",
            "carpet/train/good/195.png\n",
            "carpet/train/good/196.png\n",
            "carpet/train/good/197.png\n",
            "carpet/train/good/198.png\n",
            "carpet/train/good/199.png\n",
            "carpet/train/good/200.png\n",
            "carpet/train/good/201.png\n",
            "carpet/train/good/202.png\n",
            "carpet/train/good/203.png\n",
            "carpet/train/good/204.png\n",
            "carpet/train/good/205.png\n",
            "carpet/train/good/206.png\n",
            "carpet/train/good/207.png\n",
            "carpet/train/good/208.png\n",
            "carpet/train/good/209.png\n",
            "carpet/train/good/210.png\n",
            "carpet/train/good/211.png\n",
            "carpet/train/good/212.png\n",
            "carpet/train/good/213.png\n",
            "carpet/train/good/214.png\n",
            "carpet/train/good/215.png\n",
            "carpet/train/good/216.png\n",
            "carpet/train/good/217.png\n",
            "carpet/train/good/218.png\n",
            "carpet/train/good/219.png\n",
            "carpet/train/good/220.png\n",
            "carpet/train/good/221.png\n",
            "carpet/train/good/222.png\n",
            "carpet/train/good/223.png\n",
            "carpet/train/good/224.png\n",
            "carpet/train/good/225.png\n",
            "carpet/train/good/226.png\n",
            "carpet/train/good/227.png\n",
            "carpet/train/good/228.png\n",
            "carpet/train/good/229.png\n",
            "carpet/train/good/230.png\n",
            "carpet/train/good/231.png\n",
            "carpet/train/good/232.png\n",
            "carpet/train/good/233.png\n",
            "carpet/train/good/234.png\n",
            "carpet/train/good/235.png\n",
            "carpet/train/good/236.png\n",
            "carpet/train/good/237.png\n",
            "carpet/train/good/238.png\n",
            "carpet/train/good/239.png\n",
            "carpet/train/good/240.png\n",
            "carpet/train/good/241.png\n",
            "carpet/train/good/242.png\n",
            "carpet/train/good/243.png\n",
            "carpet/train/good/244.png\n",
            "carpet/train/good/245.png\n",
            "carpet/train/good/246.png\n",
            "carpet/train/good/247.png\n",
            "carpet/train/good/248.png\n",
            "carpet/train/good/249.png\n",
            "carpet/train/good/250.png\n",
            "carpet/train/good/251.png\n",
            "carpet/train/good/252.png\n",
            "carpet/train/good/253.png\n",
            "carpet/train/good/254.png\n",
            "carpet/train/good/255.png\n",
            "carpet/train/good/256.png\n",
            "carpet/train/good/257.png\n",
            "carpet/train/good/258.png\n",
            "carpet/train/good/259.png\n",
            "carpet/train/good/260.png\n",
            "carpet/train/good/261.png\n",
            "carpet/train/good/262.png\n",
            "carpet/train/good/263.png\n",
            "carpet/train/good/264.png\n",
            "carpet/train/good/266.png\n",
            "carpet/train/good/267.png\n",
            "carpet/train/good/268.png\n",
            "carpet/train/good/269.png\n",
            "carpet/train/good/270.png\n",
            "carpet/train/good/271.png\n",
            "carpet/train/good/272.png\n",
            "carpet/train/good/273.png\n",
            "carpet/train/good/274.png\n",
            "carpet/train/good/275.png\n",
            "carpet/train/good/276.png\n",
            "carpet/train/good/277.png\n",
            "carpet/train/good/278.png\n",
            "carpet/train/good/279.png\n",
            "carpet/train/good/032.png\n",
            "carpet/train/good/033.png\n",
            "carpet/train/good/034.png\n",
            "carpet/train/good/035.png\n",
            "carpet/train/good/036.png\n",
            "carpet/train/good/037.png\n",
            "carpet/train/good/038.png\n",
            "carpet/train/good/039.png\n",
            "carpet/train/good/040.png\n",
            "carpet/train/good/041.png\n",
            "carpet/train/good/042.png\n",
            "carpet/train/good/043.png\n",
            "carpet/train/good/044.png\n",
            "carpet/train/good/045.png\n",
            "carpet/train/good/046.png\n",
            "carpet/train/good/047.png\n",
            "carpet/train/good/048.png\n",
            "carpet/train/good/049.png\n",
            "carpet/train/good/050.png\n",
            "carpet/train/good/051.png\n",
            "carpet/train/good/052.png\n",
            "carpet/train/good/053.png\n",
            "carpet/train/good/054.png\n",
            "carpet/train/good/055.png\n",
            "carpet/train/good/056.png\n",
            "carpet/train/good/057.png\n",
            "carpet/train/good/058.png\n",
            "carpet/train/good/059.png\n",
            "carpet/train/good/060.png\n",
            "carpet/train/good/061.png\n",
            "carpet/train/good/062.png\n",
            "carpet/train/good/063.png\n",
            "carpet/train/good/064.png\n",
            "carpet/train/good/065.png\n",
            "carpet/train/good/066.png\n",
            "carpet/train/good/067.png\n",
            "carpet/train/good/068.png\n",
            "carpet/train/good/069.png\n",
            "carpet/train/good/070.png\n",
            "carpet/train/good/071.png\n",
            "carpet/train/good/072.png\n",
            "carpet/train/good/073.png\n",
            "carpet/train/good/074.png\n",
            "carpet/train/good/075.png\n",
            "carpet/train/good/076.png\n",
            "carpet/train/good/077.png\n",
            "carpet/train/good/078.png\n",
            "carpet/train/good/079.png\n",
            "carpet/train/good/080.png\n",
            "carpet/train/good/081.png\n",
            "carpet/train/good/082.png\n",
            "carpet/train/good/083.png\n",
            "carpet/train/good/084.png\n",
            "carpet/train/good/085.png\n",
            "carpet/train/good/086.png\n",
            "carpet/train/good/087.png\n",
            "carpet/train/good/088.png\n",
            "carpet/train/good/089.png\n",
            "carpet/train/good/090.png\n",
            "carpet/train/good/091.png\n",
            "carpet/train/good/092.png\n",
            "carpet/train/good/093.png\n",
            "carpet/train/good/094.png\n",
            "carpet/train/good/095.png\n",
            "carpet/train/good/096.png\n",
            "carpet/train/good/097.png\n",
            "carpet/train/good/098.png\n",
            "carpet/train/good/099.png\n",
            "carpet/train/good/100.png\n",
            "carpet/train/good/101.png\n",
            "carpet/train/good/102.png\n",
            "carpet/train/good/103.png\n",
            "carpet/train/good/104.png\n",
            "carpet/train/good/105.png\n",
            "carpet/train/good/106.png\n",
            "carpet/train/good/107.png\n",
            "carpet/train/good/108.png\n",
            "carpet/train/good/109.png\n",
            "carpet/train/good/110.png\n",
            "carpet/train/good/111.png\n",
            "carpet/train/good/112.png\n",
            "carpet/train/good/113.png\n",
            "carpet/train/good/114.png\n",
            "carpet/train/good/115.png\n",
            "carpet/train/good/116.png\n",
            "carpet/train/good/117.png\n",
            "carpet/train/good/118.png\n",
            "carpet/train/good/119.png\n",
            "carpet/train/good/120.png\n",
            "carpet/train/good/121.png\n",
            "carpet/train/good/122.png\n",
            "carpet/train/good/123.png\n",
            "carpet/train/good/124.png\n",
            "carpet/train/good/125.png\n",
            "carpet/train/good/126.png\n",
            "carpet/train/good/127.png\n",
            "carpet/train/good/128.png\n",
            "carpet/train/good/129.png\n",
            "carpet/train/good/265.png\n",
            "carpet/train/good/031.png\n",
            "carpet/readme.txt\n",
            "carpet/license.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "# 이거 폴더 안의 모든 거에 대한 경로가 다 담겨져있음 리스트형태로\n",
        "\n",
        "# ground_truth 파일 경로\n",
        "# data_dir_ground_color = glob.glob(\"/content/carpet/ground_truth/color/*\")\n",
        "# data_dir_ground_cut = glob.glob(\"/content/carpet/ground_truth/cut/*\")\n",
        "# data_dir_ground_hole = glob.glob(\"/content/carpet/ground_truth/hole/*\")\n",
        "# data_dir_ground_metal_contam = glob.glob(\"/content/carpet/ground_truth/metal_contamination/*\")\n",
        "# data_dir_ground_thread = glob.glob(\"/content/carpet/ground_truth/thread/*\")\n",
        "\n",
        "# test 파일 경로\n",
        "data_dir_test_color = glob.glob(\"/content/carpet/test/color/*\")\n",
        "data_dir_test_cut = glob.glob(\"/content/carpet/test/cut/*\")\n",
        "data_dir_test_good = glob.glob(\"/content/carpet/test/good/*\")\n",
        "data_dir_test_hole = glob.glob(\"/content/carpet/test/hole/*\")\n",
        "data_dir_test_metal_contam = glob.glob(\"/content/carpet/test/metal_contamination/*\")\n",
        "data_dir_test_thread = glob.glob(\"/content/carpet/test/thread/*\")\n",
        "\n",
        "# train 파일 경로\n",
        "data_dir_train_good = glob.glob(\"/content/carpet/train/good/*\")"
      ],
      "metadata": {
        "id": "3doNKwWKqQh8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# img = Image.open(data_dir_test_color[12])\n",
        "# img\n",
        "# import numpy as np\n",
        "# np_image = np.array(img)\n",
        "# np_image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3M4-dUEudP3",
        "outputId": "4b1a0bff-d3be-4830-d51c-32b56546a1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1024, 1024, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Combine all the file paths in a list\n",
        "file_paths_train = data_dir_train_good\n",
        "list_file_path_test = [data_dir_test_color, data_dir_test_cut, data_dir_test_good,data_dir_test_hole,data_dir_test_metal_contam,data_dir_test_thread ]\n",
        "\n",
        "train_image_list = []\n",
        "test_color_image_list = []\n",
        "test_cut_image_list = []\n",
        "test_good_image_list =[]\n",
        "test_hole_image_list = []\n",
        "test_metal_contam_image_list = []\n",
        "test_thread_image_list = []\n",
        "\n",
        "all_list_for_test = [test_color_image_list, test_cut_image_list, test_good_image_list, test_hole_image_list, test_metal_contam_image_list, test_thread_image_list]\n",
        "\n",
        "for image in data_dir_train_good:\n",
        "    # Open the image using PIL\n",
        "    img = Image.open(image)\n",
        "\n",
        "    # Resize the image to (512, 512)\n",
        "    img = img.resize((512, 512))\n",
        "\n",
        "    # Convert the image to a NumPy array and scale pixel values to range 0-1\n",
        "    np_image = np.array(img) / 255.0\n",
        "\n",
        "    train_image_list.append(np_image)\n",
        "\n",
        "# Iterate over each class-specific list and file path list\n",
        "for class_list, file_paths in zip(all_list_for_test, list_file_path_test):\n",
        "    # Iterate over each file path in the current class\n",
        "    for file_path in file_paths:\n",
        "        # Open the image using PIL\n",
        "        img = Image.open(file_path)\n",
        "        # Resize the image to (512, 512)\n",
        "        img = img.resize((512, 512))\n",
        "        # Convert the image to a NumPy array\n",
        "        np_image = np.array(img) / 255.0\n",
        "\n",
        "        # Append the NumPy array to the current class-specific list\n",
        "        class_list.append(np_image)"
      ],
      "metadata": {
        "id": "xQOHj1FRq7f6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a list of dataframes and corresponding target labels\n",
        "dataframes = [test_color_image_list, test_cut_image_list, test_hole_image_list,\n",
        "              test_metal_contam_image_list, test_thread_image_list, test_good_image_list]\n",
        "\n",
        "target_labels = [1, 1, 1, 1, 1, 0]\n",
        "\n",
        "# Create dataframes and concatenate them\n",
        "dfs = [pd.DataFrame({'image': images, 'target': [target] * len(images)}) for images, target in zip(dataframes, target_labels)]\n",
        "df_test = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Print the combined dataframe\n",
        "print(df_test)\n",
        "\n",
        "# Create a DataFrame for the train data\n",
        "df_train = pd.DataFrame({'image': train_image_list, 'target': [0] * len(train_image_list)})\n",
        "\n",
        "print(df_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLdBqbJu3EX0",
        "outputId": "29dc149a-5ae9-4953-c981-6cf2b17bd753"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 image  target\n",
            "0    [[[0.1607843137254902, 0.1568627450980392, 0.1...       1\n",
            "1    [[[0.2823529411764706, 0.2549019607843137, 0.2...       1\n",
            "2    [[[0.12549019607843137, 0.13333333333333333, 0...       1\n",
            "3    [[[0.16470588235294117, 0.15294117647058825, 0...       1\n",
            "4    [[[0.3137254901960784, 0.2784313725490196, 0.2...       1\n",
            "..                                                 ...     ...\n",
            "112  [[[0.2980392156862745, 0.2901960784313726, 0.2...       0\n",
            "113  [[[0.21176470588235294, 0.1803921568627451, 0....       0\n",
            "114  [[[0.25098039215686274, 0.24313725490196078, 0...       0\n",
            "115  [[[0.3058823529411765, 0.29411764705882354, 0....       0\n",
            "116  [[[0.21568627450980393, 0.1843137254901961, 0....       0\n",
            "\n",
            "[117 rows x 2 columns]\n",
            "                                                 image  target\n",
            "0    [[[0.22745098039215686, 0.20784313725490197, 0...       0\n",
            "1    [[[0.44313725490196076, 0.4549019607843137, 0....       0\n",
            "2    [[[0.2549019607843137, 0.23921568627450981, 0....       0\n",
            "3    [[[0.38823529411764707, 0.396078431372549, 0.4...       0\n",
            "4    [[[0.30980392156862746, 0.32941176470588235, 0...       0\n",
            "..                                                 ...     ...\n",
            "275  [[[0.2823529411764706, 0.27450980392156865, 0....       0\n",
            "276  [[[0.30196078431372547, 0.3058823529411765, 0....       0\n",
            "277  [[[0.3058823529411765, 0.3176470588235294, 0.3...       0\n",
            "278  [[[0.2901960784313726, 0.28627450980392155, 0....       0\n",
            "279  [[[0.37254901960784315, 0.3215686274509804, 0....       0\n",
            "\n",
            "[280 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df_test.copy()\n",
        "df2 = df_train.copy()"
      ],
      "metadata": {
        "id": "os7usfugEfzC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the dataframe to a CSV file\n",
        "# df.to_csv('/content/mvtec_carpet_final_df.csv', index=False)\n",
        "# import shutil\n",
        "# shutil.move('/content/mvtec_carpet_final_df.csv', '/content/drive/MyDrive/')"
      ],
      "metadata": {
        "id": "2BtFY4rlHJ_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = df_train['image']\n",
        "y_train = df_train['target']\n",
        "X_test = df_test['image']\n",
        "y_test = df_test['target']\n",
        "\n",
        "# print(X_train)\n",
        "# print(y_train)\n",
        "# print(X_test)\n",
        "# print(y_test)\n",
        "\n",
        "print(type(X_train))\n",
        "print(type(X_test))\n",
        "\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(df2['image'], df2['target'], test_size=0.2, stratify=df2['target'], random_state = 1024)\n",
        "\n",
        "\n",
        "# change the series object into a tensor object so we can input it into a model\n",
        "X_train = np.array(X_train.tolist())\n",
        "X_test = np.array(X_test.tolist())\n",
        "y_train = np.array(y_train.tolist())\n",
        "y_test = np.array(y_test.tolist())\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "type(X_train)"
      ],
      "metadata": {
        "id": "kMvMdkGAO8XM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7922cdc-41e3-4c98-c26d-21b1c59b8f27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "input_shape = (512, 512, 3)\n",
        "\n",
        "# Encoder\n",
        "encoder_input = tf.keras.Input(shape=input_shape)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoder_input)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "# Bottleneck layer\n",
        "encoded = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "\n",
        "# Decoder\n",
        "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "\n",
        "# Dropout layer to prevent overfitting\n",
        "x = layers.Dropout(0.2)(x)\n",
        "\n",
        "decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# Autoencoder model\n",
        "autoencoder = models.Model(encoder_input, decoded)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "autoencoder.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "sWsuyTLjedxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b2a7d5-059b-4f98-994d-6981c8e7b91b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 512, 512, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 512, 512, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 256, 256, 32)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 128, 128, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 64, 64, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 32, 32, 256)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 32, 32, 256)       1179904   \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 64, 64, 256)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 64, 64, 128)       295040    \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 128, 128, 128)    0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 128, 128, 64)      73792     \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 256, 256, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 256, 256, 32)      18464     \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSampling  (None, 512, 512, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512, 512, 32)      0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 512, 512, 3)       867       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,136,643\n",
            "Trainable params: 3,136,643\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_train contains your training data\n",
        "autoencoder.fit(X_train, X_train, epochs=300, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXB1BRbgYAli",
        "outputId": "7948ce5f-2906-4fb0-ff16-b1f9c5187f71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "18/18 [==============================] - 9s 488ms/step - loss: 0.6248 - accuracy: 0.6223\n",
            "Epoch 2/300\n",
            "18/18 [==============================] - 9s 496ms/step - loss: 0.6251 - accuracy: 0.6294\n",
            "Epoch 3/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6248 - accuracy: 0.6202\n",
            "Epoch 4/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6247 - accuracy: 0.6366\n",
            "Epoch 5/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6244 - accuracy: 0.6276\n",
            "Epoch 6/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6243 - accuracy: 0.6365\n",
            "Epoch 7/300\n",
            "18/18 [==============================] - 9s 489ms/step - loss: 0.6242 - accuracy: 0.6348\n",
            "Epoch 8/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6240 - accuracy: 0.6363\n",
            "Epoch 9/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6242 - accuracy: 0.6419\n",
            "Epoch 10/300\n",
            "18/18 [==============================] - 9s 486ms/step - loss: 0.6240 - accuracy: 0.6420\n",
            "Epoch 11/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6237 - accuracy: 0.6426\n",
            "Epoch 12/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6239 - accuracy: 0.6425\n",
            "Epoch 13/300\n",
            "18/18 [==============================] - 9s 493ms/step - loss: 0.6236 - accuracy: 0.6460\n",
            "Epoch 14/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6234 - accuracy: 0.6503\n",
            "Epoch 15/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6238 - accuracy: 0.6470\n",
            "Epoch 16/300\n",
            "18/18 [==============================] - 9s 491ms/step - loss: 0.6236 - accuracy: 0.6492\n",
            "Epoch 17/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6233 - accuracy: 0.6525\n",
            "Epoch 18/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6231 - accuracy: 0.6530\n",
            "Epoch 19/300\n",
            "18/18 [==============================] - 9s 493ms/step - loss: 0.6234 - accuracy: 0.6571\n",
            "Epoch 20/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6230 - accuracy: 0.6560\n",
            "Epoch 21/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6228 - accuracy: 0.6582\n",
            "Epoch 22/300\n",
            "18/18 [==============================] - 9s 492ms/step - loss: 0.6231 - accuracy: 0.6594\n",
            "Epoch 23/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6227 - accuracy: 0.6585\n",
            "Epoch 24/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6229 - accuracy: 0.6667\n",
            "Epoch 25/300\n",
            "18/18 [==============================] - 9s 491ms/step - loss: 0.6226 - accuracy: 0.6595\n",
            "Epoch 26/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6224 - accuracy: 0.6661\n",
            "Epoch 27/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6225 - accuracy: 0.6660\n",
            "Epoch 28/300\n",
            "18/18 [==============================] - 9s 491ms/step - loss: 0.6223 - accuracy: 0.6658\n",
            "Epoch 29/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6222 - accuracy: 0.6656\n",
            "Epoch 30/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6229 - accuracy: 0.6700\n",
            "Epoch 31/300\n",
            "18/18 [==============================] - 9s 496ms/step - loss: 0.6225 - accuracy: 0.6683\n",
            "Epoch 32/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6222 - accuracy: 0.6699\n",
            "Epoch 33/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6220 - accuracy: 0.6742\n",
            "Epoch 34/300\n",
            "18/18 [==============================] - 9s 496ms/step - loss: 0.6216 - accuracy: 0.6697\n",
            "Epoch 35/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6219 - accuracy: 0.6727\n",
            "Epoch 36/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6217 - accuracy: 0.6705\n",
            "Epoch 37/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6215 - accuracy: 0.6763\n",
            "Epoch 38/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6210 - accuracy: 0.6664\n",
            "Epoch 39/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6211 - accuracy: 0.6684\n",
            "Epoch 40/300\n",
            "18/18 [==============================] - 9s 495ms/step - loss: 0.6207 - accuracy: 0.6670\n",
            "Epoch 41/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6201 - accuracy: 0.6704\n",
            "Epoch 42/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6198 - accuracy: 0.6654\n",
            "Epoch 43/300\n",
            "18/18 [==============================] - 9s 491ms/step - loss: 0.6198 - accuracy: 0.6749\n",
            "Epoch 44/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6195 - accuracy: 0.6683\n",
            "Epoch 45/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6194 - accuracy: 0.6721\n",
            "Epoch 46/300\n",
            "18/18 [==============================] - 9s 491ms/step - loss: 0.6192 - accuracy: 0.6736\n",
            "Epoch 47/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6192 - accuracy: 0.6763\n",
            "Epoch 48/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6192 - accuracy: 0.6735\n",
            "Epoch 49/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6190 - accuracy: 0.6793\n",
            "Epoch 50/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6190 - accuracy: 0.6797\n",
            "Epoch 51/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6188 - accuracy: 0.6789\n",
            "Epoch 52/300\n",
            "18/18 [==============================] - 9s 496ms/step - loss: 0.6188 - accuracy: 0.6800\n",
            "Epoch 53/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6185 - accuracy: 0.6810\n",
            "Epoch 54/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6191 - accuracy: 0.6837\n",
            "Epoch 55/300\n",
            "18/18 [==============================] - 9s 495ms/step - loss: 0.6186 - accuracy: 0.6806\n",
            "Epoch 56/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6185 - accuracy: 0.6817\n",
            "Epoch 57/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6184 - accuracy: 0.6825\n",
            "Epoch 58/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6183 - accuracy: 0.6845\n",
            "Epoch 59/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6183 - accuracy: 0.6826\n",
            "Epoch 60/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6182 - accuracy: 0.6845\n",
            "Epoch 61/300\n",
            "18/18 [==============================] - 9s 496ms/step - loss: 0.6182 - accuracy: 0.6848\n",
            "Epoch 62/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6181 - accuracy: 0.6847\n",
            "Epoch 63/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6181 - accuracy: 0.6869\n",
            "Epoch 64/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6180 - accuracy: 0.6845\n",
            "Epoch 65/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6179 - accuracy: 0.6889\n",
            "Epoch 66/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6182 - accuracy: 0.6857\n",
            "Epoch 67/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6181 - accuracy: 0.6897\n",
            "Epoch 68/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6185 - accuracy: 0.6888\n",
            "Epoch 69/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6179 - accuracy: 0.6873\n",
            "Epoch 70/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6177 - accuracy: 0.6911\n",
            "Epoch 71/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6176 - accuracy: 0.6912\n",
            "Epoch 72/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6183 - accuracy: 0.6917\n",
            "Epoch 73/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6176 - accuracy: 0.6928\n",
            "Epoch 74/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6176 - accuracy: 0.6933\n",
            "Epoch 75/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6175 - accuracy: 0.6936\n",
            "Epoch 76/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6174 - accuracy: 0.6926\n",
            "Epoch 77/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6177 - accuracy: 0.6969\n",
            "Epoch 78/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6174 - accuracy: 0.6948\n",
            "Epoch 79/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6175 - accuracy: 0.6964\n",
            "Epoch 80/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6173 - accuracy: 0.6953\n",
            "Epoch 81/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6173 - accuracy: 0.6977\n",
            "Epoch 82/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6172 - accuracy: 0.6966\n",
            "Epoch 83/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6174 - accuracy: 0.6955\n",
            "Epoch 84/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6173 - accuracy: 0.6991\n",
            "Epoch 85/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6172 - accuracy: 0.6991\n",
            "Epoch 86/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6172 - accuracy: 0.6971\n",
            "Epoch 87/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6172 - accuracy: 0.7004\n",
            "Epoch 88/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6172 - accuracy: 0.6996\n",
            "Epoch 89/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6170 - accuracy: 0.6995\n",
            "Epoch 90/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6172 - accuracy: 0.6989\n",
            "Epoch 91/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6171 - accuracy: 0.7005\n",
            "Epoch 92/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6170 - accuracy: 0.7012\n",
            "Epoch 93/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6170 - accuracy: 0.7019\n",
            "Epoch 94/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6170 - accuracy: 0.7011\n",
            "Epoch 95/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6169 - accuracy: 0.7038\n",
            "Epoch 96/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6170 - accuracy: 0.7016\n",
            "Epoch 97/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6169 - accuracy: 0.7034\n",
            "Epoch 98/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6169 - accuracy: 0.7013\n",
            "Epoch 99/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6168 - accuracy: 0.7041\n",
            "Epoch 100/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6170 - accuracy: 0.7028\n",
            "Epoch 101/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6169 - accuracy: 0.7015\n",
            "Epoch 102/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6169 - accuracy: 0.7039\n",
            "Epoch 103/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6167 - accuracy: 0.7043\n",
            "Epoch 104/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6169 - accuracy: 0.7033\n",
            "Epoch 105/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6169 - accuracy: 0.7037\n",
            "Epoch 106/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6168 - accuracy: 0.7032\n",
            "Epoch 107/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6166 - accuracy: 0.7049\n",
            "Epoch 108/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6167 - accuracy: 0.7054\n",
            "Epoch 109/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6166 - accuracy: 0.7063\n",
            "Epoch 110/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6167 - accuracy: 0.7042\n",
            "Epoch 111/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6166 - accuracy: 0.7074\n",
            "Epoch 112/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6165 - accuracy: 0.7046\n",
            "Epoch 113/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6165 - accuracy: 0.7074\n",
            "Epoch 114/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6165 - accuracy: 0.7051\n",
            "Epoch 115/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6166 - accuracy: 0.7078\n",
            "Epoch 116/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6164 - accuracy: 0.7056\n",
            "Epoch 117/300\n",
            "18/18 [==============================] - 9s 518ms/step - loss: 0.6167 - accuracy: 0.7084\n",
            "Epoch 118/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6165 - accuracy: 0.7071\n",
            "Epoch 119/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6166 - accuracy: 0.7064\n",
            "Epoch 120/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6165 - accuracy: 0.7077\n",
            "Epoch 121/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6165 - accuracy: 0.7075\n",
            "Epoch 122/300\n",
            "18/18 [==============================] - 9s 496ms/step - loss: 0.6164 - accuracy: 0.7081\n",
            "Epoch 123/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6165 - accuracy: 0.7079\n",
            "Epoch 124/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6163 - accuracy: 0.7082\n",
            "Epoch 125/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6163 - accuracy: 0.7093\n",
            "Epoch 126/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6162 - accuracy: 0.7087\n",
            "Epoch 127/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6163 - accuracy: 0.7090\n",
            "Epoch 128/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6163 - accuracy: 0.7080\n",
            "Epoch 129/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6162 - accuracy: 0.7096\n",
            "Epoch 130/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6163 - accuracy: 0.7099\n",
            "Epoch 131/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6163 - accuracy: 0.7105\n",
            "Epoch 132/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6162 - accuracy: 0.7095\n",
            "Epoch 133/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6164 - accuracy: 0.7093\n",
            "Epoch 134/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6163 - accuracy: 0.7103\n",
            "Epoch 135/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6162 - accuracy: 0.7108\n",
            "Epoch 136/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6161 - accuracy: 0.7080\n",
            "Epoch 137/300\n",
            "18/18 [==============================] - 9s 496ms/step - loss: 0.6162 - accuracy: 0.7108\n",
            "Epoch 138/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6161 - accuracy: 0.7122\n",
            "Epoch 139/300\n",
            "18/18 [==============================] - 9s 520ms/step - loss: 0.6163 - accuracy: 0.7085\n",
            "Epoch 140/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6163 - accuracy: 0.7124\n",
            "Epoch 141/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6161 - accuracy: 0.7092\n",
            "Epoch 142/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6161 - accuracy: 0.7116\n",
            "Epoch 143/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6161 - accuracy: 0.7113\n",
            "Epoch 144/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6160 - accuracy: 0.7110\n",
            "Epoch 145/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6162 - accuracy: 0.7109\n",
            "Epoch 146/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6160 - accuracy: 0.7127\n",
            "Epoch 147/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6161 - accuracy: 0.7107\n",
            "Epoch 148/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6160 - accuracy: 0.7125\n",
            "Epoch 149/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6159 - accuracy: 0.7129\n",
            "Epoch 150/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6164 - accuracy: 0.7127\n",
            "Epoch 151/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6160 - accuracy: 0.7109\n",
            "Epoch 152/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6159 - accuracy: 0.7131\n",
            "Epoch 153/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6158 - accuracy: 0.7128\n",
            "Epoch 154/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6160 - accuracy: 0.7123\n",
            "Epoch 155/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6159 - accuracy: 0.7134\n",
            "Epoch 156/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6159 - accuracy: 0.7130\n",
            "Epoch 157/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6160 - accuracy: 0.7132\n",
            "Epoch 158/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6159 - accuracy: 0.7138\n",
            "Epoch 159/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6160 - accuracy: 0.7132\n",
            "Epoch 160/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6159 - accuracy: 0.7136\n",
            "Epoch 161/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6159 - accuracy: 0.7123\n",
            "Epoch 162/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6158 - accuracy: 0.7151\n",
            "Epoch 163/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6158 - accuracy: 0.7134\n",
            "Epoch 164/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6158 - accuracy: 0.7147\n",
            "Epoch 165/300\n",
            "18/18 [==============================] - 9s 497ms/step - loss: 0.6159 - accuracy: 0.7146\n",
            "Epoch 166/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6157 - accuracy: 0.7140\n",
            "Epoch 167/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6159 - accuracy: 0.7144\n",
            "Epoch 168/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6158 - accuracy: 0.7137\n",
            "Epoch 169/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6158 - accuracy: 0.7147\n",
            "Epoch 170/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6157 - accuracy: 0.7158\n",
            "Epoch 171/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6160 - accuracy: 0.7129\n",
            "Epoch 172/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6158 - accuracy: 0.7144\n",
            "Epoch 173/300\n",
            "18/18 [==============================] - 9s 519ms/step - loss: 0.6157 - accuracy: 0.7155\n",
            "Epoch 174/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6158 - accuracy: 0.7153\n",
            "Epoch 175/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6157 - accuracy: 0.7156\n",
            "Epoch 176/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6156 - accuracy: 0.7159\n",
            "Epoch 177/300\n",
            "18/18 [==============================] - 9s 495ms/step - loss: 0.6158 - accuracy: 0.7155\n",
            "Epoch 178/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6157 - accuracy: 0.7162\n",
            "Epoch 179/300\n",
            "18/18 [==============================] - 9s 521ms/step - loss: 0.6156 - accuracy: 0.7166\n",
            "Epoch 180/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6158 - accuracy: 0.7146\n",
            "Epoch 181/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6156 - accuracy: 0.7169\n",
            "Epoch 182/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6156 - accuracy: 0.7169\n",
            "Epoch 183/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6158 - accuracy: 0.7162\n",
            "Epoch 184/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6157 - accuracy: 0.7166\n",
            "Epoch 185/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6158 - accuracy: 0.7175\n",
            "Epoch 186/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6158 - accuracy: 0.7154\n",
            "Epoch 187/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6156 - accuracy: 0.7164\n",
            "Epoch 188/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6156 - accuracy: 0.7172\n",
            "Epoch 189/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6157 - accuracy: 0.7167\n",
            "Epoch 190/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6155 - accuracy: 0.7170\n",
            "Epoch 191/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6157 - accuracy: 0.7177\n",
            "Epoch 192/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6156 - accuracy: 0.7171\n",
            "Epoch 193/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6157 - accuracy: 0.7187\n",
            "Epoch 194/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6156 - accuracy: 0.7154\n",
            "Epoch 195/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6155 - accuracy: 0.7175\n",
            "Epoch 196/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6154 - accuracy: 0.7193\n",
            "Epoch 197/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6155 - accuracy: 0.7179\n",
            "Epoch 198/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6154 - accuracy: 0.7181\n",
            "Epoch 199/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6154 - accuracy: 0.7175\n",
            "Epoch 200/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6155 - accuracy: 0.7196\n",
            "Epoch 201/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6155 - accuracy: 0.7189\n",
            "Epoch 202/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6154 - accuracy: 0.7177\n",
            "Epoch 203/300\n",
            "18/18 [==============================] - 9s 516ms/step - loss: 0.6155 - accuracy: 0.7182\n",
            "Epoch 204/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6154 - accuracy: 0.7193\n",
            "Epoch 205/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6156 - accuracy: 0.7185\n",
            "Epoch 206/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6154 - accuracy: 0.7188\n",
            "Epoch 207/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6155 - accuracy: 0.7196\n",
            "Epoch 208/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6156 - accuracy: 0.7188\n",
            "Epoch 209/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6154 - accuracy: 0.7192\n",
            "Epoch 210/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6156 - accuracy: 0.7182\n",
            "Epoch 211/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6156 - accuracy: 0.7198\n",
            "Epoch 212/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6154 - accuracy: 0.7189\n",
            "Epoch 213/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6154 - accuracy: 0.7197\n",
            "Epoch 214/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6154 - accuracy: 0.7193\n",
            "Epoch 215/300\n",
            "18/18 [==============================] - 9s 517ms/step - loss: 0.6153 - accuracy: 0.7207\n",
            "Epoch 216/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6153 - accuracy: 0.7205\n",
            "Epoch 217/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6153 - accuracy: 0.7198\n",
            "Epoch 218/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6155 - accuracy: 0.7207\n",
            "Epoch 219/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6154 - accuracy: 0.7208\n",
            "Epoch 220/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6153 - accuracy: 0.7196\n",
            "Epoch 221/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6154 - accuracy: 0.7209\n",
            "Epoch 222/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6153 - accuracy: 0.7207\n",
            "Epoch 223/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6153 - accuracy: 0.7210\n",
            "Epoch 224/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6154 - accuracy: 0.7190\n",
            "Epoch 225/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6153 - accuracy: 0.7200\n",
            "Epoch 226/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6152 - accuracy: 0.7214\n",
            "Epoch 227/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6153 - accuracy: 0.7217\n",
            "Epoch 228/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6152 - accuracy: 0.7207\n",
            "Epoch 229/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6152 - accuracy: 0.7216\n",
            "Epoch 230/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6153 - accuracy: 0.7214\n",
            "Epoch 231/300\n",
            "18/18 [==============================] - 9s 516ms/step - loss: 0.6152 - accuracy: 0.7212\n",
            "Epoch 232/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6152 - accuracy: 0.7222\n",
            "Epoch 233/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6152 - accuracy: 0.7214\n",
            "Epoch 234/300\n",
            "18/18 [==============================] - 9s 517ms/step - loss: 0.6156 - accuracy: 0.7208\n",
            "Epoch 235/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6153 - accuracy: 0.7204\n",
            "Epoch 236/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6151 - accuracy: 0.7224\n",
            "Epoch 237/300\n",
            "18/18 [==============================] - 9s 520ms/step - loss: 0.6152 - accuracy: 0.7209\n",
            "Epoch 238/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6152 - accuracy: 0.7216\n",
            "Epoch 239/300\n",
            "18/18 [==============================] - 9s 498ms/step - loss: 0.6151 - accuracy: 0.7223\n",
            "Epoch 240/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6151 - accuracy: 0.7226\n",
            "Epoch 241/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6152 - accuracy: 0.7219\n",
            "Epoch 242/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6151 - accuracy: 0.7235\n",
            "Epoch 243/300\n",
            "18/18 [==============================] - 9s 518ms/step - loss: 0.6153 - accuracy: 0.7224\n",
            "Epoch 244/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6154 - accuracy: 0.7215\n",
            "Epoch 245/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6152 - accuracy: 0.7218\n",
            "Epoch 246/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6153 - accuracy: 0.7230\n",
            "Epoch 247/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6150 - accuracy: 0.7231\n",
            "Epoch 248/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6152 - accuracy: 0.7212\n",
            "Epoch 249/300\n",
            "18/18 [==============================] - 9s 514ms/step - loss: 0.6152 - accuracy: 0.7239\n",
            "Epoch 250/300\n",
            "18/18 [==============================] - 9s 518ms/step - loss: 0.6151 - accuracy: 0.7223\n",
            "Epoch 251/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6151 - accuracy: 0.7228\n",
            "Epoch 252/300\n",
            "18/18 [==============================] - 9s 506ms/step - loss: 0.6150 - accuracy: 0.7229\n",
            "Epoch 253/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6152 - accuracy: 0.7226\n",
            "Epoch 254/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6152 - accuracy: 0.7233\n",
            "Epoch 255/300\n",
            "18/18 [==============================] - 9s 510ms/step - loss: 0.6151 - accuracy: 0.7224\n",
            "Epoch 256/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6150 - accuracy: 0.7241\n",
            "Epoch 257/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6151 - accuracy: 0.7236\n",
            "Epoch 258/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6151 - accuracy: 0.7234\n",
            "Epoch 259/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6150 - accuracy: 0.7236\n",
            "Epoch 260/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6150 - accuracy: 0.7230\n",
            "Epoch 261/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6151 - accuracy: 0.7250\n",
            "Epoch 262/300\n",
            "18/18 [==============================] - 9s 517ms/step - loss: 0.6152 - accuracy: 0.7206\n",
            "Epoch 263/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6150 - accuracy: 0.7254\n",
            "Epoch 264/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6152 - accuracy: 0.7211\n",
            "Epoch 265/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6152 - accuracy: 0.7228\n",
            "Epoch 266/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6151 - accuracy: 0.7230\n",
            "Epoch 267/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6151 - accuracy: 0.7247\n",
            "Epoch 268/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6149 - accuracy: 0.7248\n",
            "Epoch 269/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6150 - accuracy: 0.7230\n",
            "Epoch 270/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6150 - accuracy: 0.7251\n",
            "Epoch 271/300\n",
            "18/18 [==============================] - 9s 517ms/step - loss: 0.6151 - accuracy: 0.7237\n",
            "Epoch 272/300\n",
            "18/18 [==============================] - 9s 509ms/step - loss: 0.6149 - accuracy: 0.7244\n",
            "Epoch 273/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6150 - accuracy: 0.7256\n",
            "Epoch 274/300\n",
            "18/18 [==============================] - 9s 518ms/step - loss: 0.6148 - accuracy: 0.7237\n",
            "Epoch 275/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6154 - accuracy: 0.7233\n",
            "Epoch 276/300\n",
            "18/18 [==============================] - 9s 503ms/step - loss: 0.6151 - accuracy: 0.7225\n",
            "Epoch 277/300\n",
            "18/18 [==============================] - 9s 516ms/step - loss: 0.6149 - accuracy: 0.7245\n",
            "Epoch 278/300\n",
            "18/18 [==============================] - 9s 518ms/step - loss: 0.6151 - accuracy: 0.7246\n",
            "Epoch 279/300\n",
            "18/18 [==============================] - 9s 501ms/step - loss: 0.6151 - accuracy: 0.7233\n",
            "Epoch 280/300\n",
            "18/18 [==============================] - 9s 516ms/step - loss: 0.6149 - accuracy: 0.7250\n",
            "Epoch 281/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6150 - accuracy: 0.7243\n",
            "Epoch 282/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6149 - accuracy: 0.7260\n",
            "Epoch 283/300\n",
            "18/18 [==============================] - 9s 515ms/step - loss: 0.6149 - accuracy: 0.7235\n",
            "Epoch 284/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6148 - accuracy: 0.7256\n",
            "Epoch 285/300\n",
            "18/18 [==============================] - 9s 499ms/step - loss: 0.6149 - accuracy: 0.7241\n",
            "Epoch 286/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6149 - accuracy: 0.7250\n",
            "Epoch 287/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6148 - accuracy: 0.7252\n",
            "Epoch 288/300\n",
            "18/18 [==============================] - 9s 500ms/step - loss: 0.6149 - accuracy: 0.7261\n",
            "Epoch 289/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6148 - accuracy: 0.7251\n",
            "Epoch 290/300\n",
            "18/18 [==============================] - 9s 519ms/step - loss: 0.6149 - accuracy: 0.7257\n",
            "Epoch 291/300\n",
            "18/18 [==============================] - 9s 504ms/step - loss: 0.6148 - accuracy: 0.7242\n",
            "Epoch 292/300\n",
            "18/18 [==============================] - 9s 507ms/step - loss: 0.6148 - accuracy: 0.7259\n",
            "Epoch 293/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6148 - accuracy: 0.7253\n",
            "Epoch 294/300\n",
            "18/18 [==============================] - 9s 502ms/step - loss: 0.6147 - accuracy: 0.7250\n",
            "Epoch 295/300\n",
            "18/18 [==============================] - 9s 513ms/step - loss: 0.6151 - accuracy: 0.7255\n",
            "Epoch 296/300\n",
            "18/18 [==============================] - 9s 512ms/step - loss: 0.6149 - accuracy: 0.7243\n",
            "Epoch 297/300\n",
            "18/18 [==============================] - 9s 508ms/step - loss: 0.6149 - accuracy: 0.7255\n",
            "Epoch 298/300\n",
            "18/18 [==============================] - 9s 511ms/step - loss: 0.6147 - accuracy: 0.7253\n",
            "Epoch 299/300\n",
            "18/18 [==============================] - 9s 518ms/step - loss: 0.6149 - accuracy: 0.7257\n",
            "Epoch 300/300\n",
            "18/18 [==============================] - 9s 505ms/step - loss: 0.6148 - accuracy: 0.7264\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ac985b5c730>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X_test contains your test data\n",
        "loss = autoencoder.evaluate(X_test, X_test)\n",
        "\n",
        "print(f\"Test Loss: {loss}\")\n",
        "# Assuming X_test contains your test data\n",
        "reconstructions = autoencoder.predict(X_test)\n",
        "mse = np.mean(np.square(X_test - reconstructions))\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "from keras import metrics\n",
        "\n",
        "loss, accuracy = autoencoder.evaluate(X_test, X_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uco3V93aQn5",
        "outputId": "4dca6a62-37ee-4b43-c818-a4381bc85c7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 6s 2s/step - loss: 0.6211 - accuracy: 0.8090\n",
            "Test Loss: [0.6210716962814331, 0.8089950680732727]\n",
            "4/4 [==============================] - 1s 140ms/step\n",
            "Mean Squared Error (MSE): 0.003011921187862754\n",
            "4/4 [==============================] - 1s 223ms/step - loss: 0.6211 - accuracy: 0.8090\n",
            "Test Loss: 0.6210716962814331\n",
            "Test Accuracy: 0.8089950680732727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejMc1sT3d7US",
        "outputId": "61c92394-ff54-4e35-d1f3-fdd7174519ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 123ms/step\n",
            "Mean Squared Error (MSE): 0.004134712275117636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-Pg3vUeeAD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classify_all.fit(X_train,y_train,validation_data=(X_test,y_test), epochs=100)"
      ],
      "metadata": {
        "id": "MUI9aGCa7eTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이번엔 그냥 정상 비정상으로만 분류하는거 도전해보기\n",
        "simple_df = df.copy()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Replace non-zero values in the 'target' column with 1\n",
        "simple_df.loc[simple_df['target'] != 0, 'target'] = 1\n",
        "\n",
        "# Print the updated DataFrame\n",
        "simple_df['target'].value_counts()"
      ],
      "metadata": {
        "id": "oiZ_LxSj86NA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32500988-780c-4b35-8ae0-b6c6152842a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    308\n",
              "1     89\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(simple_df['image'], simple_df['target'], test_size=0.2, stratify=simple_df['target'])"
      ],
      "metadata": {
        "id": "V5p45Xwyhxj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJyK6Do0laqO",
        "outputId": "7ce263ae-0529-4423-cb5d-82005e9012f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    246\n",
              "1     71\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X1_train = X1_train.to_numpy()\n",
        "X1_test = X1_test.to_numpy()\n",
        "y1_train = y1_train.to_numpy()\n",
        "y1_test = y1_test.to_numpy()\n",
        "\n",
        "print(type(X1_train))\n",
        "\n",
        "X1_train = np.array(X1_train.tolist())\n",
        "X1_test = np.array(X1_test.tolist())\n",
        "y1_train = np.array(y1_train.tolist())\n",
        "y1_test = np.array(y1_test.tolist())\n",
        "\n",
        "print(type(X1_train))\n",
        "\n",
        "X1_train = tf.convert_to_tensor(X1_train, dtype=tf.float32)\n",
        "X1_test = tf.convert_to_tensor(X1_test, dtype=tf.float32)\n",
        "y1_train = tf.convert_to_tensor(y1_train, dtype=tf.float32)\n",
        "y1_test = tf.convert_to_tensor(y1_test, dtype=tf.float32)\n",
        "\n",
        "print(type(X1_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hkd1BGYj04k",
        "outputId": "d7fd45d6-93c4-49bf-c45c-b45fa5c6d712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# # Define the model\n",
        "# model = models.Sequential()\n",
        "\n",
        "# # Convolutional layers\n",
        "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3), padding='same'))\n",
        "# model.add(layers.MaxPooling2D((2, 2),padding='same'))\n",
        "# model.add(layers.Dropout(0.2))  # Add Dropout with 20% rate\n",
        "\n",
        "# model.add(layers.Conv2D(64, (3, 3), activation='relu' , padding= 'same'))\n",
        "# model.add(layers.Dropout(0.2))  # Add Dropout with 20% rate\n",
        "\n",
        "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "# model.add(layers.Dropout(0.2))  # Add Dropout with 20% rate\n",
        "\n",
        "# model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
        "# model.add(layers.MaxPooling2D((2, 2)))\n",
        "# model.add(layers.Dropout(0.2))  # Add Dropout with 20% rate\n",
        "\n",
        "# model.add(layers.Conv2D(512, (3, 3), activation='relu'))  # New convolutional layer\n",
        "# model.add(layers.Dropout(0.2))  # Add Dropout with 20% rate\n",
        "\n",
        "# # Flatten the feature maps\n",
        "# model.add(layers.Flatten())\n",
        "\n",
        "# # Dense layers\n",
        "# model.add(layers.Dense(128, activation='relu'))\n",
        "# model.add(layers.Dropout(0.5))  # Add Dropout with 50% rate\n",
        "# model.add(layers.Dense(64, activation='relu'))\n",
        "# model.add(layers.Dropout(0.5))  # Add Dropout with 50% rate\n",
        "# model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # Print the model summary\n",
        "# model.summary()\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
        "model.add(layers.Dropout(0.2))  # Add Dropout with 20% rate\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2), padding= 'same'))\n",
        "model.add(layers.Dropout(0.2))  # Add Dropout with 20% rate\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flatten the feature maps\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV6_1Eotvfbk",
        "outputId": "fa886e11-445f-40bf-80f5-791da04d491b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 254, 254, 32)      0         \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 252, 252, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 126, 126, 64)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 126, 126, 64)      0         \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 124, 124, 128)     73856     \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 62, 62, 128)      0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 492032)            0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 64)                31490112  \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,583,425\n",
            "Trainable params: 31,583,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "history = model.fit(X1_train, y1_train,\n",
        "                    validation_data=(X1_test, y1_test),\n",
        "                    epochs=200,\n",
        "                    )\n",
        "\n",
        "# Find the best model based on validation accuracy\n",
        "best_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
        "best_validation_accuracy = max(history.history['val_accuracy'])\n",
        "print(f\"Best model is at epoch {best_epoch} with validation accuracy of {best_validation_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zw7ZazU-kSuQ",
        "outputId": "1ad4ed75-7679-4ad1-9f08-eba2b78d4189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "10/10 [==============================] - 15s 866ms/step - loss: 5.4371 - accuracy: 0.6246 - val_loss: 0.5347 - val_accuracy: 0.7750\n",
            "Epoch 2/200\n",
            "10/10 [==============================] - 3s 298ms/step - loss: 0.5478 - accuracy: 0.7760 - val_loss: 0.5344 - val_accuracy: 0.7750\n",
            "Epoch 3/200\n",
            "10/10 [==============================] - 3s 295ms/step - loss: 0.5554 - accuracy: 0.7760 - val_loss: 0.6071 - val_accuracy: 0.7750\n",
            "Epoch 4/200\n",
            "10/10 [==============================] - 3s 293ms/step - loss: 0.5286 - accuracy: 0.7760 - val_loss: 0.5559 - val_accuracy: 0.7750\n",
            "Epoch 5/200\n",
            "10/10 [==============================] - 3s 294ms/step - loss: 0.5548 - accuracy: 0.7760 - val_loss: 0.5233 - val_accuracy: 0.7750\n",
            "Epoch 6/200\n",
            "10/10 [==============================] - 3s 299ms/step - loss: 0.5636 - accuracy: 0.7760 - val_loss: 0.6502 - val_accuracy: 0.7750\n",
            "Epoch 7/200\n",
            "10/10 [==============================] - 3s 298ms/step - loss: 0.5604 - accuracy: 0.7760 - val_loss: 0.5218 - val_accuracy: 0.7750\n",
            "Epoch 8/200\n",
            "10/10 [==============================] - 3s 300ms/step - loss: 0.5363 - accuracy: 0.7760 - val_loss: 0.5536 - val_accuracy: 0.7750\n",
            "Epoch 9/200\n",
            "10/10 [==============================] - 3s 296ms/step - loss: 0.5236 - accuracy: 0.7760 - val_loss: 0.5653 - val_accuracy: 0.7750\n",
            "Epoch 10/200\n",
            "10/10 [==============================] - 3s 298ms/step - loss: 0.5246 - accuracy: 0.7760 - val_loss: 0.5895 - val_accuracy: 0.7750\n",
            "Epoch 11/200\n",
            "10/10 [==============================] - 3s 302ms/step - loss: 0.5149 - accuracy: 0.7760 - val_loss: 0.5203 - val_accuracy: 0.7750\n",
            "Epoch 12/200\n",
            "10/10 [==============================] - 3s 300ms/step - loss: 0.5086 - accuracy: 0.7760 - val_loss: 0.5220 - val_accuracy: 0.7750\n",
            "Epoch 13/200\n",
            "10/10 [==============================] - 3s 297ms/step - loss: 0.5015 - accuracy: 0.7760 - val_loss: 0.4801 - val_accuracy: 0.7750\n",
            "Epoch 14/200\n",
            "10/10 [==============================] - 3s 298ms/step - loss: 0.4988 - accuracy: 0.7760 - val_loss: 0.4804 - val_accuracy: 0.7750\n",
            "Epoch 15/200\n",
            "10/10 [==============================] - 3s 303ms/step - loss: 0.4829 - accuracy: 0.7760 - val_loss: 0.4909 - val_accuracy: 0.7750\n",
            "Epoch 16/200\n",
            "10/10 [==============================] - 3s 300ms/step - loss: 0.5696 - accuracy: 0.7760 - val_loss: 0.6825 - val_accuracy: 0.7750\n",
            "Epoch 17/200\n",
            "10/10 [==============================] - 3s 295ms/step - loss: 0.6720 - accuracy: 0.7760 - val_loss: 0.6516 - val_accuracy: 0.7750\n",
            "Epoch 18/200\n",
            "10/10 [==============================] - 3s 296ms/step - loss: 0.5856 - accuracy: 0.7760 - val_loss: 0.5529 - val_accuracy: 0.7750\n",
            "Epoch 19/200\n",
            "10/10 [==============================] - 3s 295ms/step - loss: 0.5304 - accuracy: 0.7760 - val_loss: 0.5776 - val_accuracy: 0.7750\n",
            "Epoch 20/200\n",
            "10/10 [==============================] - 3s 301ms/step - loss: 0.4874 - accuracy: 0.7760 - val_loss: 0.4843 - val_accuracy: 0.7750\n",
            "Epoch 21/200\n",
            "10/10 [==============================] - 3s 295ms/step - loss: 0.4942 - accuracy: 0.7760 - val_loss: 0.4696 - val_accuracy: 0.7750\n",
            "Epoch 22/200\n",
            "10/10 [==============================] - 3s 296ms/step - loss: 0.5410 - accuracy: 0.7760 - val_loss: 0.5087 - val_accuracy: 0.7750\n",
            "Epoch 23/200\n",
            "10/10 [==============================] - 3s 293ms/step - loss: 0.5481 - accuracy: 0.7760 - val_loss: 0.5977 - val_accuracy: 0.7750\n",
            "Epoch 24/200\n",
            "10/10 [==============================] - 3s 296ms/step - loss: 0.5054 - accuracy: 0.7760 - val_loss: 0.5197 - val_accuracy: 0.7750\n",
            "Epoch 25/200\n",
            "10/10 [==============================] - 3s 301ms/step - loss: 0.4787 - accuracy: 0.7760 - val_loss: 0.4961 - val_accuracy: 0.7750\n",
            "Epoch 26/200\n",
            "10/10 [==============================] - 3s 295ms/step - loss: 0.4423 - accuracy: 0.7760 - val_loss: 0.4605 - val_accuracy: 0.7750\n",
            "Epoch 27/200\n",
            "10/10 [==============================] - 3s 294ms/step - loss: 0.4233 - accuracy: 0.7760 - val_loss: 0.4730 - val_accuracy: 0.7750\n",
            "Epoch 28/200\n",
            "10/10 [==============================] - 3s 296ms/step - loss: 0.4002 - accuracy: 0.7760 - val_loss: 0.4327 - val_accuracy: 0.7750\n",
            "Epoch 29/200\n",
            "10/10 [==============================] - 3s 301ms/step - loss: 0.4001 - accuracy: 0.7760 - val_loss: 0.5257 - val_accuracy: 0.7750\n",
            "Epoch 30/200\n",
            "10/10 [==============================] - 3s 297ms/step - loss: 0.3854 - accuracy: 0.7855 - val_loss: 0.4384 - val_accuracy: 0.7750\n",
            "Epoch 31/200\n",
            "10/10 [==============================] - 3s 298ms/step - loss: 0.3105 - accuracy: 0.8580 - val_loss: 0.4322 - val_accuracy: 0.7875\n",
            "Epoch 32/200\n",
            "10/10 [==============================] - 3s 299ms/step - loss: 0.2372 - accuracy: 0.9117 - val_loss: 0.4608 - val_accuracy: 0.7750\n",
            "Epoch 33/200\n",
            "10/10 [==============================] - 3s 302ms/step - loss: 0.1825 - accuracy: 0.9338 - val_loss: 0.4524 - val_accuracy: 0.7500\n",
            "Epoch 34/200\n",
            "10/10 [==============================] - 3s 308ms/step - loss: 0.1362 - accuracy: 0.9685 - val_loss: 0.5672 - val_accuracy: 0.7750\n",
            "Epoch 35/200\n",
            "10/10 [==============================] - 3s 303ms/step - loss: 0.0864 - accuracy: 0.9874 - val_loss: 0.5596 - val_accuracy: 0.7125\n",
            "Epoch 36/200\n",
            "10/10 [==============================] - 3s 302ms/step - loss: 0.0945 - accuracy: 0.9558 - val_loss: 0.7870 - val_accuracy: 0.7750\n",
            "Epoch 37/200\n",
            "10/10 [==============================] - 3s 300ms/step - loss: 0.0442 - accuracy: 0.9968 - val_loss: 0.7234 - val_accuracy: 0.7750\n",
            "Epoch 38/200\n",
            "10/10 [==============================] - 3s 307ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 0.7115 - val_accuracy: 0.6875\n",
            "Epoch 39/200\n",
            "10/10 [==============================] - 3s 304ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.9378 - val_accuracy: 0.7500\n",
            "Epoch 40/200\n",
            "10/10 [==============================] - 3s 303ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 1.0089 - val_accuracy: 0.7500\n",
            "Epoch 41/200\n",
            "10/10 [==============================] - 3s 305ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.9405 - val_accuracy: 0.6750\n",
            "Epoch 42/200\n",
            "10/10 [==============================] - 3s 311ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.0224 - val_accuracy: 0.7000\n",
            "Epoch 43/200\n",
            "10/10 [==============================] - 3s 329ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.1457 - val_accuracy: 0.7500\n",
            "Epoch 44/200\n",
            "10/10 [==============================] - 3s 301ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.0627 - val_accuracy: 0.6875\n",
            "Epoch 45/200\n",
            "10/10 [==============================] - 3s 305ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0774 - val_accuracy: 0.6875\n",
            "Epoch 46/200\n",
            "10/10 [==============================] - 3s 304ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.1665 - val_accuracy: 0.7500\n",
            "Epoch 47/200\n",
            "10/10 [==============================] - 3s 308ms/step - loss: 9.7415e-04 - accuracy: 1.0000 - val_loss: 1.1116 - val_accuracy: 0.6875\n",
            "Epoch 48/200\n",
            "10/10 [==============================] - 3s 306ms/step - loss: 6.9736e-04 - accuracy: 1.0000 - val_loss: 1.1932 - val_accuracy: 0.7375\n",
            "Epoch 49/200\n",
            "10/10 [==============================] - 3s 305ms/step - loss: 6.2458e-04 - accuracy: 1.0000 - val_loss: 1.1630 - val_accuracy: 0.6875\n",
            "Epoch 50/200\n",
            "10/10 [==============================] - 3s 302ms/step - loss: 5.3509e-04 - accuracy: 1.0000 - val_loss: 1.1757 - val_accuracy: 0.6875\n",
            "Epoch 51/200\n",
            "10/10 [==============================] - 3s 302ms/step - loss: 5.8731e-04 - accuracy: 1.0000 - val_loss: 1.1827 - val_accuracy: 0.6875\n",
            "Epoch 52/200\n",
            "10/10 [==============================] - 3s 310ms/step - loss: 5.7461e-04 - accuracy: 1.0000 - val_loss: 1.2217 - val_accuracy: 0.7250\n",
            "Epoch 53/200\n",
            "10/10 [==============================] - 3s 308ms/step - loss: 4.8686e-04 - accuracy: 1.0000 - val_loss: 1.2017 - val_accuracy: 0.6875\n",
            "Epoch 54/200\n",
            "10/10 [==============================] - 3s 306ms/step - loss: 3.6316e-04 - accuracy: 1.0000 - val_loss: 1.2617 - val_accuracy: 0.7250\n",
            "Epoch 55/200\n",
            "10/10 [==============================] - 3s 306ms/step - loss: 4.2389e-04 - accuracy: 1.0000 - val_loss: 1.2246 - val_accuracy: 0.7000\n",
            "Epoch 56/200\n",
            "10/10 [==============================] - 3s 309ms/step - loss: 3.8279e-04 - accuracy: 1.0000 - val_loss: 1.2389 - val_accuracy: 0.7000\n",
            "Epoch 57/200\n",
            "10/10 [==============================] - 3s 307ms/step - loss: 2.9860e-04 - accuracy: 1.0000 - val_loss: 1.2738 - val_accuracy: 0.7125\n",
            "Epoch 58/200\n",
            "10/10 [==============================] - 3s 306ms/step - loss: 3.2248e-04 - accuracy: 1.0000 - val_loss: 1.2567 - val_accuracy: 0.7000\n",
            "Epoch 59/200\n",
            " 1/10 [==>...........................] - ETA: 2s - loss: 4.5316e-04 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-397d62a4a8fe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(X1_train, y1_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1689\u001b[0m                             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m                             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1691\u001b[0;31m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1692\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \"\"\"\n\u001b[1;32m   1159\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1124\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BSCigCqonSQp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}