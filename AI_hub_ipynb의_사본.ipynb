{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf \n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9C2divoqrooB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "resistivity = pd.read_csv(\"C:\\\\Users\\\\swims\\\\Coding\\\\.ipynb_checkpoints\\\\SW캠프_대충_보고서\\\\Resistivity_data_set.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5vust-UBKwY"
      },
      "source": [
        "##### 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'Number', 'X', 'Y', 'Al', 'Ti', 'Cr', 'Fe', 'Co', 'Ni',\n",
            "       'Cu', 'Zr', 'Mo', 'W', 'Mn', 'Si', 'Mg', 'Resistance', 'Thickness',\n",
            "       'Resistivity', 'Ex_resistivity', 'ravg', 'delta', 'dHmix', 'ENavg',\n",
            "       'dEN', 'N', 'Compo'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(resistivity.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Xnvt478ir_L0"
      },
      "outputs": [],
      "source": [
        "resistivity.drop([\"Unnamed: 0\", \"Number\",\"Al\",\"Cr\",\"Fe\",\"Co\",\"Ni\",\"Zr\",\"Mo\",\"W\",\"Mn\",\"Si\",\"N\",\"Compo\" ], axis='columns', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2ac1CA1NsBTk"
      },
      "outputs": [],
      "source": [
        "X_data = resistivity.iloc[:,[0,1,2,3,4,5,6,9,10,11,12,13]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDKMegXXx4nT",
        "outputId": "94076a34-e314-4746-ba6e-16806453e0dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6548, 12)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iq5ATrigsrVl"
      },
      "outputs": [],
      "source": [
        "Y_data = resistivity.iloc[:,[7,8]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "hhVOcrvbuney",
        "outputId": "04497b60-1889-4d80-876e-fdde3828b9dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Resistivity</th>\n",
              "      <th>Ex_resistivity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30.553231</td>\n",
              "      <td>23.831108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.681781</td>\n",
              "      <td>24.212788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>90.658141</td>\n",
              "      <td>70.003844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>87.521931</td>\n",
              "      <td>67.340687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84.786582</td>\n",
              "      <td>65.078390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6543</th>\n",
              "      <td>167.729596</td>\n",
              "      <td>157.509173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6544</th>\n",
              "      <td>169.257832</td>\n",
              "      <td>159.237264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6545</th>\n",
              "      <td>171.227321</td>\n",
              "      <td>161.406609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6546</th>\n",
              "      <td>169.862215</td>\n",
              "      <td>160.241359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6547</th>\n",
              "      <td>165.504149</td>\n",
              "      <td>156.083148</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6548 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Resistivity  Ex_resistivity\n",
              "0       30.553231       23.831108\n",
              "1       30.681781       24.212788\n",
              "2       90.658141       70.003844\n",
              "3       87.521931       67.340687\n",
              "4       84.786582       65.078390\n",
              "...           ...             ...\n",
              "6543   167.729596      157.509173\n",
              "6544   169.257832      159.237264\n",
              "6545   171.227321      161.406609\n",
              "6546   169.862215      160.241359\n",
              "6547   165.504149      156.083148\n",
              "\n",
              "[6548 rows x 2 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Ti</th>\n",
              "      <th>Cu</th>\n",
              "      <th>Mg</th>\n",
              "      <th>Resistance</th>\n",
              "      <th>Thickness</th>\n",
              "      <th>ravg</th>\n",
              "      <th>delta</th>\n",
              "      <th>dHmix</th>\n",
              "      <th>ENavg</th>\n",
              "      <th>dEN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-5</td>\n",
              "      <td>39</td>\n",
              "      <td>12.509605</td>\n",
              "      <td>87.490395</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.075</td>\n",
              "      <td>899.28569</td>\n",
              "      <td>0.133004</td>\n",
              "      <td>0.099494</td>\n",
              "      <td>-3.940093</td>\n",
              "      <td>1.864973</td>\n",
              "      <td>0.092632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-5</td>\n",
              "      <td>41</td>\n",
              "      <td>11.881834</td>\n",
              "      <td>88.118166</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.075</td>\n",
              "      <td>903.06935</td>\n",
              "      <td>0.132753</td>\n",
              "      <td>0.097497</td>\n",
              "      <td>-3.769220</td>\n",
              "      <td>1.866731</td>\n",
              "      <td>0.090601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-3</td>\n",
              "      <td>-41</td>\n",
              "      <td>47.061894</td>\n",
              "      <td>52.938106</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.242</td>\n",
              "      <td>826.97664</td>\n",
              "      <td>0.146825</td>\n",
              "      <td>0.135981</td>\n",
              "      <td>-8.968923</td>\n",
              "      <td>1.768227</td>\n",
              "      <td>0.139758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-3</td>\n",
              "      <td>-39</td>\n",
              "      <td>45.888707</td>\n",
              "      <td>54.111293</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.235</td>\n",
              "      <td>822.14956</td>\n",
              "      <td>0.146355</td>\n",
              "      <td>0.136191</td>\n",
              "      <td>-8.939150</td>\n",
              "      <td>1.771512</td>\n",
              "      <td>0.139526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3</td>\n",
              "      <td>-37</td>\n",
              "      <td>44.715521</td>\n",
              "      <td>55.284479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.229</td>\n",
              "      <td>817.32248</td>\n",
              "      <td>0.145886</td>\n",
              "      <td>0.136325</td>\n",
              "      <td>-8.899467</td>\n",
              "      <td>1.774797</td>\n",
              "      <td>0.139216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6543</th>\n",
              "      <td>41</td>\n",
              "      <td>3</td>\n",
              "      <td>20.112704</td>\n",
              "      <td>63.936747</td>\n",
              "      <td>15.950549</td>\n",
              "      <td>0.369</td>\n",
              "      <td>1003.42550</td>\n",
              "      <td>0.370837</td>\n",
              "      <td>0.621207</td>\n",
              "      <td>-3.800003</td>\n",
              "      <td>1.749576</td>\n",
              "      <td>0.220601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6544</th>\n",
              "      <td>41</td>\n",
              "      <td>5</td>\n",
              "      <td>19.592147</td>\n",
              "      <td>64.086973</td>\n",
              "      <td>16.320880</td>\n",
              "      <td>0.367</td>\n",
              "      <td>1018.08610</td>\n",
              "      <td>0.376080</td>\n",
              "      <td>0.626673</td>\n",
              "      <td>-3.728841</td>\n",
              "      <td>1.748849</td>\n",
              "      <td>0.222096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6545</th>\n",
              "      <td>41</td>\n",
              "      <td>7</td>\n",
              "      <td>19.071590</td>\n",
              "      <td>64.237200</td>\n",
              "      <td>16.691210</td>\n",
              "      <td>0.366</td>\n",
              "      <td>1032.74660</td>\n",
              "      <td>0.381323</td>\n",
              "      <td>0.631991</td>\n",
              "      <td>-3.659717</td>\n",
              "      <td>1.748121</td>\n",
              "      <td>0.223578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6546</th>\n",
              "      <td>41</td>\n",
              "      <td>9</td>\n",
              "      <td>18.551033</td>\n",
              "      <td>64.387426</td>\n",
              "      <td>17.061541</td>\n",
              "      <td>0.358</td>\n",
              "      <td>1047.40720</td>\n",
              "      <td>0.386566</td>\n",
              "      <td>0.637167</td>\n",
              "      <td>-3.592631</td>\n",
              "      <td>1.747394</td>\n",
              "      <td>0.225048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6547</th>\n",
              "      <td>41</td>\n",
              "      <td>11</td>\n",
              "      <td>18.030477</td>\n",
              "      <td>64.537652</td>\n",
              "      <td>17.431871</td>\n",
              "      <td>0.344</td>\n",
              "      <td>1062.06780</td>\n",
              "      <td>0.391809</td>\n",
              "      <td>0.642207</td>\n",
              "      <td>-3.527583</td>\n",
              "      <td>1.746667</td>\n",
              "      <td>0.226506</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6548 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       X   Y         Ti         Cu         Mg  Resistance   Thickness  \\\n",
              "0     -5  39  12.509605  87.490395   0.000000       0.075   899.28569   \n",
              "1     -5  41  11.881834  88.118166   0.000000       0.075   903.06935   \n",
              "2     -3 -41  47.061894  52.938106   0.000000       0.242   826.97664   \n",
              "3     -3 -39  45.888707  54.111293   0.000000       0.235   822.14956   \n",
              "4     -3 -37  44.715521  55.284479   0.000000       0.229   817.32248   \n",
              "...   ..  ..        ...        ...        ...         ...         ...   \n",
              "6543  41   3  20.112704  63.936747  15.950549       0.369  1003.42550   \n",
              "6544  41   5  19.592147  64.086973  16.320880       0.367  1018.08610   \n",
              "6545  41   7  19.071590  64.237200  16.691210       0.366  1032.74660   \n",
              "6546  41   9  18.551033  64.387426  17.061541       0.358  1047.40720   \n",
              "6547  41  11  18.030477  64.537652  17.431871       0.344  1062.06780   \n",
              "\n",
              "          ravg     delta     dHmix     ENavg       dEN  \n",
              "0     0.133004  0.099494 -3.940093  1.864973  0.092632  \n",
              "1     0.132753  0.097497 -3.769220  1.866731  0.090601  \n",
              "2     0.146825  0.135981 -8.968923  1.768227  0.139758  \n",
              "3     0.146355  0.136191 -8.939150  1.771512  0.139526  \n",
              "4     0.145886  0.136325 -8.899467  1.774797  0.139216  \n",
              "...        ...       ...       ...       ...       ...  \n",
              "6543  0.370837  0.621207 -3.800003  1.749576  0.220601  \n",
              "6544  0.376080  0.626673 -3.728841  1.748849  0.222096  \n",
              "6545  0.381323  0.631991 -3.659717  1.748121  0.223578  \n",
              "6546  0.386566  0.637167 -3.592631  1.747394  0.225048  \n",
              "6547  0.391809  0.642207 -3.527583  1.746667  0.226506  \n",
              "\n",
              "[6548 rows x 12 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVGq7fYuBIRG"
      },
      "source": [
        "##### 모델링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vdovqTneurAm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-DzmY01xScb",
        "outputId": "ea0c8a49-7998-4d2b-8889-c165d3190de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 128)               1664      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,882\n",
            "Trainable params: 12,722\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n",
            "Epoch 1/500\n",
            "144/144 [==============================] - 1s 4ms/step - loss: 4328.6157 - r_squared: -1.1715 - val_loss: 3774.5845 - val_r_squared: -0.9577\n",
            "Epoch 2/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 3864.6538 - r_squared: -0.9178 - val_loss: 3401.1584 - val_r_squared: -0.7798\n",
            "Epoch 3/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 3252.0796 - r_squared: -0.6255 - val_loss: 2272.8145 - val_r_squared: -0.1894\n",
            "Epoch 4/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 2547.7002 - r_squared: -0.2428 - val_loss: 3543.6025 - val_r_squared: -0.9359\n",
            "Epoch 5/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 1914.1870 - r_squared: 0.0633 - val_loss: 2126.8892 - val_r_squared: -0.1560\n",
            "Epoch 6/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 1349.2798 - r_squared: 0.3541 - val_loss: 1335.4507 - val_r_squared: 0.2669\n",
            "Epoch 7/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 942.1960 - r_squared: 0.5507 - val_loss: 798.4962 - val_r_squared: 0.5536\n",
            "Epoch 8/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 638.4465 - r_squared: 0.6986 - val_loss: 420.2157 - val_r_squared: 0.7612\n",
            "Epoch 9/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 455.0551 - r_squared: 0.7799 - val_loss: 328.9970 - val_r_squared: 0.8124\n",
            "Epoch 10/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 323.3325 - r_squared: 0.8479 - val_loss: 72.4992 - val_r_squared: 0.9604\n",
            "Epoch 11/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 220.5559 - r_squared: 0.8955 - val_loss: 43.8966 - val_r_squared: 0.9754\n",
            "Epoch 12/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 163.1123 - r_squared: 0.8831 - val_loss: 97.3587 - val_r_squared: 0.9449\n",
            "Epoch 13/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 126.5397 - r_squared: 0.9355 - val_loss: 49.1461 - val_r_squared: 0.9734\n",
            "Epoch 14/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 135.8824 - r_squared: 0.9303 - val_loss: 44.6578 - val_r_squared: 0.9748\n",
            "Epoch 15/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 122.9162 - r_squared: 0.9352 - val_loss: 24.9873 - val_r_squared: 0.9862\n",
            "Epoch 16/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 111.1970 - r_squared: 0.9417 - val_loss: 20.3458 - val_r_squared: 0.9889\n",
            "Epoch 17/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 117.3421 - r_squared: 0.9324 - val_loss: 69.4901 - val_r_squared: 0.9605\n",
            "Epoch 18/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 99.4745 - r_squared: 0.9464 - val_loss: 29.2990 - val_r_squared: 0.9839\n",
            "Epoch 19/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 103.1757 - r_squared: 0.9432 - val_loss: 12.8212 - val_r_squared: 0.9928\n",
            "Epoch 20/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 92.4287 - r_squared: 0.9493 - val_loss: 52.2683 - val_r_squared: 0.9710\n",
            "Epoch 21/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 101.1651 - r_squared: 0.9410 - val_loss: 55.9622 - val_r_squared: 0.9691\n",
            "Epoch 22/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 104.8401 - r_squared: 0.9434 - val_loss: 64.5892 - val_r_squared: 0.9641\n",
            "Epoch 23/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 75.7029 - r_squared: 0.9445 - val_loss: 15.5982 - val_r_squared: 0.9913\n",
            "Epoch 24/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 87.5138 - r_squared: 0.9399 - val_loss: 23.1899 - val_r_squared: 0.9871\n",
            "Epoch 25/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 83.4970 - r_squared: 0.9505 - val_loss: 36.8164 - val_r_squared: 0.9794\n",
            "Epoch 26/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 88.5144 - r_squared: 0.9518 - val_loss: 12.2663 - val_r_squared: 0.9932\n",
            "Epoch 27/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 96.0904 - r_squared: 0.9352 - val_loss: 19.6476 - val_r_squared: 0.9890\n",
            "Epoch 28/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 104.4533 - r_squared: 0.9393 - val_loss: 44.7225 - val_r_squared: 0.9748\n",
            "Epoch 29/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 82.3493 - r_squared: 0.9519 - val_loss: 166.7762 - val_r_squared: 0.9095\n",
            "Epoch 30/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 98.0997 - r_squared: 0.9440 - val_loss: 43.6218 - val_r_squared: 0.9751\n",
            "Epoch 31/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 85.1978 - r_squared: 0.9507 - val_loss: 11.2842 - val_r_squared: 0.9937\n",
            "Epoch 32/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 82.3365 - r_squared: 0.9466 - val_loss: 26.8061 - val_r_squared: 0.9853\n",
            "Epoch 33/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 87.3423 - r_squared: 0.9525 - val_loss: 51.5083 - val_r_squared: 0.9719\n",
            "Epoch 34/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 93.0419 - r_squared: 0.9478 - val_loss: 12.1702 - val_r_squared: 0.9933\n",
            "Epoch 35/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 87.0614 - r_squared: 0.9491 - val_loss: 41.2352 - val_r_squared: 0.9768\n",
            "Epoch 36/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 90.0165 - r_squared: 0.9480 - val_loss: 15.3256 - val_r_squared: 0.9914\n",
            "Epoch 37/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 87.6150 - r_squared: 0.9453 - val_loss: 145.0575 - val_r_squared: 0.9186\n",
            "Epoch 38/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 87.9687 - r_squared: 0.9488 - val_loss: 24.9636 - val_r_squared: 0.9862\n",
            "Epoch 39/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 89.7847 - r_squared: 0.9433 - val_loss: 6.0913 - val_r_squared: 0.9966\n",
            "Epoch 40/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 76.7491 - r_squared: 0.9559 - val_loss: 24.5560 - val_r_squared: 0.9864\n",
            "Epoch 41/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 66.0412 - r_squared: 0.9635 - val_loss: 82.1617 - val_r_squared: 0.9535\n",
            "Epoch 42/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 68.6970 - r_squared: 0.9622 - val_loss: 14.2154 - val_r_squared: 0.9918\n",
            "Epoch 43/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 82.4397 - r_squared: 0.9495 - val_loss: 39.4784 - val_r_squared: 0.9777\n",
            "Epoch 44/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 84.6082 - r_squared: 0.9469 - val_loss: 23.5971 - val_r_squared: 0.9867\n",
            "Epoch 45/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 77.4190 - r_squared: 0.9226 - val_loss: 40.5357 - val_r_squared: 0.9786\n",
            "Epoch 46/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 84.1168 - r_squared: 0.9525 - val_loss: 46.8810 - val_r_squared: 0.9736\n",
            "Epoch 47/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 80.7562 - r_squared: 0.9517 - val_loss: 9.7077 - val_r_squared: 0.9946\n",
            "Epoch 48/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 86.2655 - r_squared: 0.9553 - val_loss: 31.5805 - val_r_squared: 0.9823\n",
            "Epoch 49/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 73.2873 - r_squared: 0.9569 - val_loss: 6.7957 - val_r_squared: 0.9962\n",
            "Epoch 50/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 81.1514 - r_squared: 0.9523 - val_loss: 35.9152 - val_r_squared: 0.9809\n",
            "Epoch 51/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 84.6708 - r_squared: 0.9507 - val_loss: 34.9359 - val_r_squared: 0.9801\n",
            "Epoch 52/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 72.2992 - r_squared: 0.9555 - val_loss: 11.4046 - val_r_squared: 0.9936\n",
            "Epoch 53/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 80.1011 - r_squared: 0.9525 - val_loss: 12.8347 - val_r_squared: 0.9928\n",
            "Epoch 54/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 84.2691 - r_squared: 0.9501 - val_loss: 13.6122 - val_r_squared: 0.9921\n",
            "Epoch 55/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 82.8229 - r_squared: 0.9474 - val_loss: 8.6576 - val_r_squared: 0.9951\n",
            "Epoch 56/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 79.8915 - r_squared: 0.9566 - val_loss: 8.0858 - val_r_squared: 0.9956\n",
            "Epoch 57/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 75.2192 - r_squared: 0.9573 - val_loss: 5.8343 - val_r_squared: 0.9969\n",
            "Epoch 58/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 60.3336 - r_squared: 0.9602 - val_loss: 9.4240 - val_r_squared: 0.9949\n",
            "Epoch 59/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 72.1101 - r_squared: 0.9583 - val_loss: 13.9666 - val_r_squared: 0.9922\n",
            "Epoch 60/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 64.0693 - r_squared: 0.9632 - val_loss: 14.8637 - val_r_squared: 0.9923\n",
            "Epoch 61/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 74.4324 - r_squared: 0.9602 - val_loss: 25.5587 - val_r_squared: 0.9860\n",
            "Epoch 62/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 77.8724 - r_squared: 0.9551 - val_loss: 8.8247 - val_r_squared: 0.9951\n",
            "Epoch 63/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 80.1767 - r_squared: 0.9546 - val_loss: 9.0153 - val_r_squared: 0.9952\n",
            "Epoch 64/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 74.0619 - r_squared: 0.9573 - val_loss: 25.1737 - val_r_squared: 0.9864\n",
            "Epoch 65/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 80.2021 - r_squared: 0.9497 - val_loss: 11.1628 - val_r_squared: 0.9939\n",
            "Epoch 66/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 71.9419 - r_squared: 0.9578 - val_loss: 5.8709 - val_r_squared: 0.9968\n",
            "Epoch 67/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 64.2803 - r_squared: 0.9539 - val_loss: 8.4657 - val_r_squared: 0.9955\n",
            "Epoch 68/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 76.1614 - r_squared: 0.9582 - val_loss: 9.8285 - val_r_squared: 0.9946\n",
            "Epoch 69/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 75.3111 - r_squared: 0.9551 - val_loss: 9.1644 - val_r_squared: 0.9950\n",
            "Epoch 70/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.9357 - r_squared: 0.9626 - val_loss: 14.2720 - val_r_squared: 0.9920\n",
            "Epoch 71/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 68.2096 - r_squared: 0.9622 - val_loss: 12.0143 - val_r_squared: 0.9933\n",
            "Epoch 72/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 73.1830 - r_squared: 0.9573 - val_loss: 24.2765 - val_r_squared: 0.9867\n",
            "Epoch 73/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 72.0602 - r_squared: 0.9551 - val_loss: 23.2904 - val_r_squared: 0.9874\n",
            "Epoch 74/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 72.4305 - r_squared: 0.9604 - val_loss: 8.7291 - val_r_squared: 0.9953\n",
            "Epoch 75/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 78.5513 - r_squared: 0.9535 - val_loss: 18.2819 - val_r_squared: 0.9905\n",
            "Epoch 76/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 69.7751 - r_squared: 0.9612 - val_loss: 30.4960 - val_r_squared: 0.9831\n",
            "Epoch 77/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 64.6194 - r_squared: 0.9630 - val_loss: 8.5590 - val_r_squared: 0.9957\n",
            "Epoch 78/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 70.4497 - r_squared: 0.9615 - val_loss: 16.5313 - val_r_squared: 0.9909\n",
            "Epoch 79/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 70.7819 - r_squared: 0.9567 - val_loss: 26.3051 - val_r_squared: 0.9856\n",
            "Epoch 80/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 81.6379 - r_squared: 0.9507 - val_loss: 4.5306 - val_r_squared: 0.9975\n",
            "Epoch 81/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 67.1113 - r_squared: 0.9598 - val_loss: 3.9184 - val_r_squared: 0.9978\n",
            "Epoch 82/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 72.0494 - r_squared: 0.9585 - val_loss: 7.1694 - val_r_squared: 0.9960\n",
            "Epoch 83/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.5578 - r_squared: 0.9571 - val_loss: 9.2326 - val_r_squared: 0.9950\n",
            "Epoch 84/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 79.4752 - r_squared: 0.9526 - val_loss: 7.5480 - val_r_squared: 0.9958\n",
            "Epoch 85/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 77.3053 - r_squared: 0.9580 - val_loss: 5.8396 - val_r_squared: 0.9968\n",
            "Epoch 86/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 64.7512 - r_squared: 0.9620 - val_loss: 11.6327 - val_r_squared: 0.9938\n",
            "Epoch 87/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 78.4862 - r_squared: 0.9538 - val_loss: 7.8365 - val_r_squared: 0.9956\n",
            "Epoch 88/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 78.6040 - r_squared: 0.9570 - val_loss: 15.9840 - val_r_squared: 0.9909\n",
            "Epoch 89/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.1406 - r_squared: 0.9662 - val_loss: 27.4775 - val_r_squared: 0.9842\n",
            "Epoch 90/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.4287 - r_squared: 0.9638 - val_loss: 11.5971 - val_r_squared: 0.9933\n",
            "Epoch 91/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 91.7084 - r_squared: 0.9399 - val_loss: 12.3073 - val_r_squared: 0.9930\n",
            "Epoch 92/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 67.7613 - r_squared: 0.9588 - val_loss: 7.2709 - val_r_squared: 0.9960\n",
            "Epoch 93/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 74.8911 - r_squared: 0.9571 - val_loss: 6.4162 - val_r_squared: 0.9965\n",
            "Epoch 94/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 70.2033 - r_squared: 0.9603 - val_loss: 2.7502 - val_r_squared: 0.9985\n",
            "Epoch 95/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 75.8139 - r_squared: 0.9586 - val_loss: 3.8391 - val_r_squared: 0.9978\n",
            "Epoch 96/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 80.8632 - r_squared: 0.9559 - val_loss: 7.9489 - val_r_squared: 0.9957\n",
            "Epoch 97/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 67.9433 - r_squared: 0.9532 - val_loss: 9.3568 - val_r_squared: 0.9949\n",
            "Epoch 98/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 76.6545 - r_squared: 0.9524 - val_loss: 14.5277 - val_r_squared: 0.9924\n",
            "Epoch 99/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 68.3434 - r_squared: 0.9624 - val_loss: 7.0256 - val_r_squared: 0.9960\n",
            "Epoch 100/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 77.2314 - r_squared: 0.9605 - val_loss: 11.9021 - val_r_squared: 0.9933\n",
            "Epoch 101/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 66.6825 - r_squared: 0.9563 - val_loss: 3.8794 - val_r_squared: 0.9979\n",
            "Epoch 102/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 70.6508 - r_squared: 0.9569 - val_loss: 8.1699 - val_r_squared: 0.9955\n",
            "Epoch 103/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 71.6568 - r_squared: 0.9556 - val_loss: 11.2153 - val_r_squared: 0.9940\n",
            "Epoch 104/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 76.2082 - r_squared: 0.9570 - val_loss: 9.9652 - val_r_squared: 0.9946\n",
            "Epoch 105/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 64.0994 - r_squared: 0.9606 - val_loss: 13.8972 - val_r_squared: 0.9926\n",
            "Epoch 106/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.7144 - r_squared: 0.9490 - val_loss: 10.2482 - val_r_squared: 0.9942\n",
            "Epoch 107/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 68.0209 - r_squared: 0.9624 - val_loss: 3.2130 - val_r_squared: 0.9982\n",
            "Epoch 108/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 77.2384 - r_squared: 0.9579 - val_loss: 7.2978 - val_r_squared: 0.9963\n",
            "Epoch 109/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 72.2565 - r_squared: 0.9544 - val_loss: 7.8915 - val_r_squared: 0.9958\n",
            "Epoch 110/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 63.0810 - r_squared: 0.9618 - val_loss: 10.2746 - val_r_squared: 0.9947\n",
            "Epoch 111/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 80.3718 - r_squared: 0.9411 - val_loss: 14.5305 - val_r_squared: 0.9919\n",
            "Epoch 112/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 70.0708 - r_squared: 0.9617 - val_loss: 13.0111 - val_r_squared: 0.9927\n",
            "Epoch 113/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 78.1294 - r_squared: 0.9537 - val_loss: 9.0400 - val_r_squared: 0.9951\n",
            "Epoch 114/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 68.0431 - r_squared: 0.9621 - val_loss: 10.0776 - val_r_squared: 0.9947\n",
            "Epoch 115/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.0060 - r_squared: 0.9494 - val_loss: 11.8499 - val_r_squared: 0.9934\n",
            "Epoch 116/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 75.8547 - r_squared: 0.9583 - val_loss: 2.9565 - val_r_squared: 0.9983\n",
            "Epoch 117/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 68.4081 - r_squared: 0.9600 - val_loss: 2.2520 - val_r_squared: 0.9987\n",
            "Epoch 118/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 65.5110 - r_squared: 0.9628 - val_loss: 6.5390 - val_r_squared: 0.9963\n",
            "Epoch 119/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 63.2134 - r_squared: 0.9670 - val_loss: 10.1251 - val_r_squared: 0.9946\n",
            "Epoch 120/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.6630 - r_squared: 0.9648 - val_loss: 11.3744 - val_r_squared: 0.9940\n",
            "Epoch 121/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 75.3100 - r_squared: 0.9569 - val_loss: 6.8391 - val_r_squared: 0.9963\n",
            "Epoch 122/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 60.1131 - r_squared: 0.9602 - val_loss: 10.9966 - val_r_squared: 0.9938\n",
            "Epoch 123/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 80.9396 - r_squared: 0.9556 - val_loss: 2.2426 - val_r_squared: 0.9988\n",
            "Epoch 124/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.3481 - r_squared: 0.9513 - val_loss: 3.2279 - val_r_squared: 0.9981\n",
            "Epoch 125/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 77.8812 - r_squared: 0.9540 - val_loss: 6.1852 - val_r_squared: 0.9965\n",
            "Epoch 126/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.1142 - r_squared: 0.9593 - val_loss: 8.2751 - val_r_squared: 0.9954\n",
            "Epoch 127/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 62.4967 - r_squared: 0.9598 - val_loss: 3.0822 - val_r_squared: 0.9984\n",
            "Epoch 128/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 74.7081 - r_squared: 0.9606 - val_loss: 16.1600 - val_r_squared: 0.9912\n",
            "Epoch 129/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 72.0897 - r_squared: 0.9574 - val_loss: 5.2383 - val_r_squared: 0.9973\n",
            "Epoch 130/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 69.5333 - r_squared: 0.9612 - val_loss: 14.1872 - val_r_squared: 0.9919\n",
            "Epoch 131/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 67.2626 - r_squared: 0.9635 - val_loss: 16.4111 - val_r_squared: 0.9910\n",
            "Epoch 132/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 70.5671 - r_squared: 0.9586 - val_loss: 10.5610 - val_r_squared: 0.9942\n",
            "Epoch 133/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 72.0835 - r_squared: 0.9558 - val_loss: 41.4206 - val_r_squared: 0.9763\n",
            "Epoch 134/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 85.4344 - r_squared: 0.9456 - val_loss: 5.7380 - val_r_squared: 0.9970\n",
            "Epoch 135/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 66.4706 - r_squared: 0.9610 - val_loss: 16.5706 - val_r_squared: 0.9911\n",
            "Epoch 136/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 69.2683 - r_squared: 0.9598 - val_loss: 6.1123 - val_r_squared: 0.9967\n",
            "Epoch 137/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 69.3229 - r_squared: 0.9610 - val_loss: 6.4279 - val_r_squared: 0.9963\n",
            "Epoch 138/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.2710 - r_squared: 0.9609 - val_loss: 2.3887 - val_r_squared: 0.9986\n",
            "Epoch 139/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 59.4702 - r_squared: 0.9678 - val_loss: 3.8357 - val_r_squared: 0.9979\n",
            "Epoch 140/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.9981 - r_squared: 0.9637 - val_loss: 5.2589 - val_r_squared: 0.9974\n",
            "Epoch 141/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 73.8884 - r_squared: 0.9535 - val_loss: 4.6831 - val_r_squared: 0.9975\n",
            "Epoch 142/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 78.5491 - r_squared: 0.9604 - val_loss: 9.8497 - val_r_squared: 0.9947\n",
            "Epoch 143/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 60.3475 - r_squared: 0.9620 - val_loss: 2.5430 - val_r_squared: 0.9986\n",
            "Epoch 144/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 74.9055 - r_squared: 0.9602 - val_loss: 7.0423 - val_r_squared: 0.9961\n",
            "Epoch 145/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 56.9101 - r_squared: 0.9682 - val_loss: 11.6364 - val_r_squared: 0.9936\n",
            "Epoch 146/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 69.8818 - r_squared: 0.9499 - val_loss: 3.9053 - val_r_squared: 0.9979\n",
            "Epoch 147/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 72.2484 - r_squared: 0.9596 - val_loss: 2.7092 - val_r_squared: 0.9984\n",
            "Epoch 148/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 64.8671 - r_squared: 0.9636 - val_loss: 2.5867 - val_r_squared: 0.9985\n",
            "Epoch 149/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 81.4640 - r_squared: 0.9545 - val_loss: 9.8251 - val_r_squared: 0.9951\n",
            "Epoch 150/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 84.2871 - r_squared: 0.9546 - val_loss: 7.3447 - val_r_squared: 0.9960\n",
            "Epoch 151/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 77.2864 - r_squared: 0.9542 - val_loss: 9.2306 - val_r_squared: 0.9949\n",
            "Epoch 152/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 68.0940 - r_squared: 0.9602 - val_loss: 13.1482 - val_r_squared: 0.9927\n",
            "Epoch 153/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 56.2863 - r_squared: 0.9668 - val_loss: 8.3129 - val_r_squared: 0.9956\n",
            "Epoch 154/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.0244 - r_squared: 0.9550 - val_loss: 19.6912 - val_r_squared: 0.9894\n",
            "Epoch 155/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 73.6582 - r_squared: 0.9559 - val_loss: 11.7248 - val_r_squared: 0.9934\n",
            "Epoch 156/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 73.6778 - r_squared: 0.9536 - val_loss: 8.6437 - val_r_squared: 0.9952\n",
            "Epoch 157/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 65.3119 - r_squared: 0.9621 - val_loss: 6.0003 - val_r_squared: 0.9967\n",
            "Epoch 158/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 64.7049 - r_squared: 0.9601 - val_loss: 5.6014 - val_r_squared: 0.9971\n",
            "Epoch 159/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 76.7545 - r_squared: 0.9567 - val_loss: 6.7408 - val_r_squared: 0.9963\n",
            "Epoch 160/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.0919 - r_squared: 0.9535 - val_loss: 13.1557 - val_r_squared: 0.9926\n",
            "Epoch 161/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 66.8433 - r_squared: 0.9608 - val_loss: 3.9122 - val_r_squared: 0.9979\n",
            "Epoch 162/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.8051 - r_squared: 0.9531 - val_loss: 9.0698 - val_r_squared: 0.9951\n",
            "Epoch 163/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.8529 - r_squared: 0.9641 - val_loss: 10.0477 - val_r_squared: 0.9946\n",
            "Epoch 164/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 66.3970 - r_squared: 0.9608 - val_loss: 2.4331 - val_r_squared: 0.9986\n",
            "Epoch 165/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 79.3453 - r_squared: 0.9483 - val_loss: 4.6328 - val_r_squared: 0.9975\n",
            "Epoch 166/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.6807 - r_squared: 0.9583 - val_loss: 7.2245 - val_r_squared: 0.9961\n",
            "Epoch 167/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 71.6900 - r_squared: 0.9546 - val_loss: 4.7667 - val_r_squared: 0.9974\n",
            "Epoch 168/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 72.2051 - r_squared: 0.9567 - val_loss: 3.0928 - val_r_squared: 0.9983\n",
            "Epoch 169/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 79.0141 - r_squared: 0.9493 - val_loss: 14.2391 - val_r_squared: 0.9926\n",
            "Epoch 170/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.1443 - r_squared: 0.9622 - val_loss: 1.8828 - val_r_squared: 0.9990\n",
            "Epoch 171/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.7465 - r_squared: 0.9636 - val_loss: 9.8754 - val_r_squared: 0.9947\n",
            "Epoch 172/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 67.8992 - r_squared: 0.9580 - val_loss: 4.8214 - val_r_squared: 0.9974\n",
            "Epoch 173/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 68.6875 - r_squared: 0.9615 - val_loss: 5.8173 - val_r_squared: 0.9967\n",
            "Epoch 174/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 75.5738 - r_squared: 0.9559 - val_loss: 4.2520 - val_r_squared: 0.9977\n",
            "Epoch 175/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 81.5031 - r_squared: 0.9507 - val_loss: 5.9946 - val_r_squared: 0.9967\n",
            "Epoch 176/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 56.0273 - r_squared: 0.9586 - val_loss: 5.0928 - val_r_squared: 0.9973\n",
            "Epoch 177/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 80.6729 - r_squared: 0.9547 - val_loss: 11.2819 - val_r_squared: 0.9938\n",
            "Epoch 178/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 83.4021 - r_squared: 0.9538 - val_loss: 4.0608 - val_r_squared: 0.9978\n",
            "Epoch 179/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 60.1912 - r_squared: 0.9669 - val_loss: 2.0638 - val_r_squared: 0.9989\n",
            "Epoch 180/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.2858 - r_squared: 0.9594 - val_loss: 8.3724 - val_r_squared: 0.9955\n",
            "Epoch 181/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.3626 - r_squared: 0.9546 - val_loss: 2.8102 - val_r_squared: 0.9985\n",
            "Epoch 182/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 77.1452 - r_squared: 0.9586 - val_loss: 2.3874 - val_r_squared: 0.9987\n",
            "Epoch 183/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.6940 - r_squared: 0.9572 - val_loss: 2.6415 - val_r_squared: 0.9986\n",
            "Epoch 184/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 63.7947 - r_squared: 0.9664 - val_loss: 5.3114 - val_r_squared: 0.9972\n",
            "Epoch 185/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 67.6479 - r_squared: 0.9614 - val_loss: 3.4999 - val_r_squared: 0.9982\n",
            "Epoch 186/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 69.2790 - r_squared: 0.9584 - val_loss: 3.2740 - val_r_squared: 0.9982\n",
            "Epoch 187/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.8134 - r_squared: 0.9544 - val_loss: 4.3934 - val_r_squared: 0.9977\n",
            "Epoch 188/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 61.9949 - r_squared: 0.9672 - val_loss: 2.1651 - val_r_squared: 0.9988\n",
            "Epoch 189/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 65.8666 - r_squared: 0.9603 - val_loss: 4.4550 - val_r_squared: 0.9977\n",
            "Epoch 190/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 74.5927 - r_squared: 0.9577 - val_loss: 4.9652 - val_r_squared: 0.9975\n",
            "Epoch 191/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 69.8808 - r_squared: 0.9623 - val_loss: 2.5877 - val_r_squared: 0.9986\n",
            "Epoch 192/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 52.4306 - r_squared: 0.9723 - val_loss: 4.0337 - val_r_squared: 0.9978\n",
            "Epoch 193/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 54.5490 - r_squared: 0.9728 - val_loss: 1.6533 - val_r_squared: 0.9991\n",
            "Epoch 194/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 85.9455 - r_squared: 0.9480 - val_loss: 11.0942 - val_r_squared: 0.9942\n",
            "Epoch 195/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 67.9582 - r_squared: 0.9649 - val_loss: 5.5558 - val_r_squared: 0.9970\n",
            "Epoch 196/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 67.9972 - r_squared: 0.9599 - val_loss: 8.0637 - val_r_squared: 0.9958\n",
            "Epoch 197/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 71.8863 - r_squared: 0.9576 - val_loss: 5.9957 - val_r_squared: 0.9970\n",
            "Epoch 198/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 60.9542 - r_squared: 0.9653 - val_loss: 4.8524 - val_r_squared: 0.9975\n",
            "Epoch 199/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 60.5225 - r_squared: 0.9668 - val_loss: 4.9615 - val_r_squared: 0.9974\n",
            "Epoch 200/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 71.4448 - r_squared: 0.9569 - val_loss: 2.8702 - val_r_squared: 0.9985\n",
            "Epoch 201/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 76.0224 - r_squared: 0.9548 - val_loss: 2.1446 - val_r_squared: 0.9988\n",
            "Epoch 202/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 81.2901 - r_squared: 0.9582 - val_loss: 4.1262 - val_r_squared: 0.9977\n",
            "Epoch 203/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 59.0694 - r_squared: 0.9662 - val_loss: 8.0556 - val_r_squared: 0.9959\n",
            "Epoch 204/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 64.1746 - r_squared: 0.9621 - val_loss: 5.8430 - val_r_squared: 0.9968\n",
            "Epoch 205/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 61.9594 - r_squared: 0.9658 - val_loss: 3.2059 - val_r_squared: 0.9983\n",
            "Epoch 206/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 72.3507 - r_squared: 0.9595 - val_loss: 2.8647 - val_r_squared: 0.9984\n",
            "Epoch 207/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 70.3012 - r_squared: 0.9540 - val_loss: 3.2888 - val_r_squared: 0.9982\n",
            "Epoch 208/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 74.1678 - r_squared: 0.9579 - val_loss: 4.5811 - val_r_squared: 0.9976\n",
            "Epoch 209/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 64.9258 - r_squared: 0.9588 - val_loss: 1.9911 - val_r_squared: 0.9989\n",
            "Epoch 210/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 70.0022 - r_squared: 0.9597 - val_loss: 3.7188 - val_r_squared: 0.9980\n",
            "Epoch 211/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 78.4710 - r_squared: 0.9538 - val_loss: 4.9218 - val_r_squared: 0.9973\n",
            "Epoch 212/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 66.0224 - r_squared: 0.9327 - val_loss: 2.5560 - val_r_squared: 0.9986\n",
            "Epoch 213/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 77.8846 - r_squared: 0.9544 - val_loss: 5.1601 - val_r_squared: 0.9974\n",
            "Epoch 214/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 71.9575 - r_squared: 0.9562 - val_loss: 15.6227 - val_r_squared: 0.9938\n",
            "Epoch 215/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 70.6920 - r_squared: 0.9599 - val_loss: 5.3239 - val_r_squared: 0.9972\n",
            "Epoch 216/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 63.0284 - r_squared: 0.9662 - val_loss: 4.8056 - val_r_squared: 0.9976\n",
            "Epoch 217/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 65.7973 - r_squared: 0.9625 - val_loss: 7.9725 - val_r_squared: 0.9959\n",
            "Epoch 218/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 52.6031 - r_squared: 0.9675 - val_loss: 16.5964 - val_r_squared: 0.9911\n",
            "Epoch 219/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 50.9976 - r_squared: 0.9724 - val_loss: 4.8706 - val_r_squared: 0.9974\n",
            "Epoch 220/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 66.7449 - r_squared: 0.9544 - val_loss: 2.6337 - val_r_squared: 0.9986\n",
            "Epoch 221/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 79.0441 - r_squared: 0.9505 - val_loss: 7.9723 - val_r_squared: 0.9959\n",
            "Epoch 222/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 61.6425 - r_squared: 0.9668 - val_loss: 1.8235 - val_r_squared: 0.9990\n",
            "Epoch 223/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 65.8485 - r_squared: 0.9638 - val_loss: 2.4658 - val_r_squared: 0.9987\n",
            "Epoch 224/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 73.3062 - r_squared: 0.9578 - val_loss: 2.0081 - val_r_squared: 0.9989\n",
            "Epoch 225/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 58.4359 - r_squared: 0.9690 - val_loss: 9.4836 - val_r_squared: 0.9952\n",
            "Epoch 226/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 54.8161 - r_squared: 0.9659 - val_loss: 4.7700 - val_r_squared: 0.9975\n",
            "Epoch 227/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 62.5711 - r_squared: 0.9623 - val_loss: 4.6080 - val_r_squared: 0.9976\n",
            "Epoch 228/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 77.9036 - r_squared: 0.9509 - val_loss: 3.3435 - val_r_squared: 0.9983\n",
            "Epoch 229/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 59.1748 - r_squared: 0.9650 - val_loss: 5.5711 - val_r_squared: 0.9970\n",
            "Epoch 230/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 54.7843 - r_squared: 0.9699 - val_loss: 2.7996 - val_r_squared: 0.9986\n",
            "Epoch 231/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 78.0768 - r_squared: 0.9502 - val_loss: 7.2011 - val_r_squared: 0.9961\n",
            "Epoch 232/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 66.1082 - r_squared: 0.9635 - val_loss: 10.6214 - val_r_squared: 0.9941\n",
            "Epoch 233/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 57.2079 - r_squared: 0.9639 - val_loss: 2.4469 - val_r_squared: 0.9987\n",
            "Epoch 234/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 57.2561 - r_squared: 0.9662 - val_loss: 4.7208 - val_r_squared: 0.9975\n",
            "Epoch 235/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 76.5489 - r_squared: 0.9510 - val_loss: 3.8100 - val_r_squared: 0.9979\n",
            "Epoch 236/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 64.5626 - r_squared: 0.9593 - val_loss: 5.5246 - val_r_squared: 0.9970\n",
            "Epoch 237/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 61.4193 - r_squared: 0.9629 - val_loss: 3.3154 - val_r_squared: 0.9982\n",
            "Epoch 238/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 76.9969 - r_squared: 0.9590 - val_loss: 5.8736 - val_r_squared: 0.9970\n",
            "Epoch 239/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 50.9897 - r_squared: 0.9706 - val_loss: 6.6304 - val_r_squared: 0.9965\n",
            "Epoch 240/500\n",
            "144/144 [==============================] - 0s 3ms/step - loss: 67.6151 - r_squared: 0.9600 - val_loss: 16.9403 - val_r_squared: 0.9908\n",
            "Epoch 241/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 75.7557 - r_squared: 0.9563 - val_loss: 4.4795 - val_r_squared: 0.9976\n",
            "Epoch 242/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 62.8292 - r_squared: 0.9594 - val_loss: 2.1200 - val_r_squared: 0.9988\n",
            "Epoch 243/500\n",
            "144/144 [==============================] - 0s 2ms/step - loss: 61.4173 - r_squared: 0.9656 - val_loss: 2.8899 - val_r_squared: 0.9984\n"
          ]
        }
      ],
      "source": [
        "# custom R-squared 함수 정의\n",
        "def r_squared(y_true, y_pred):\n",
        "    SS_res = K.sum(K.square(y_true - y_pred))\n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "    return 1 - SS_res / (SS_tot + K.epsilon())\n",
        "\n",
        "# resistivity에서 입력 데이터와 타겟 값을 추출\n",
        "input_data = resistivity.iloc[:,[0,1,2,3,4,5,6,9,10,11,12,13]].values\n",
        "target_data = resistivity.iloc[:,[7,8]].values\n",
        "\n",
        "# 데이터를 훈련 세트, 검증 세트, 테스트 세트로 나누기 (훈련 세트 70%, 검증 세트 15%, 테스트 세트 15%)\n",
        "train_input, temp_input, train_target, temp_target = train_test_split(input_data, target_data, test_size=0.3, random_state=42)\n",
        "valid_input, test_input, valid_target, test_target = train_test_split(temp_input, temp_target, test_size=0.5, random_state=42)\n",
        "\n",
        "# ANN 모델 구성\n",
        "NN = Sequential()\n",
        "NN.add(Dense(128, input_dim=12, activation='relu'))\n",
        "NN.add(Dense(64, activation='relu'))\n",
        "NN.add(BatchNormalization())\n",
        "NN.add(Dense(32, activation='relu'))\n",
        "NN.add(Dense(16, activation='relu'))\n",
        "NN.add(BatchNormalization())\n",
        "NN.add(Dense(2, activation='linear'))\n",
        "\n",
        "NN.summary()\n",
        "\n",
        "# 모델 컴파일\n",
        "NN.compile(loss='mean_squared_error', optimizer='adam', metrics=[r_squared])\n",
        "\n",
        "# Early stopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "\n",
        "# ModelCheckpoint callback to save the best model during training\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min')\n",
        "\n",
        "# 모델 학습\n",
        "history = NN.fit(train_input, train_target, epochs=500, validation_data=(valid_input, valid_target), verbose=1, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "# Load the best model saved during training\n",
        "NN.load_weights('best_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFfxaU6Nyy6U",
        "outputId": "da360d5e-3925-427c-ba43-e361c5fd3a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss(MSE): 1.7677961587905884\n",
            "Test R-squared: 0.9990302920341492\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_r_squared = NN.evaluate(test_input, test_target, verbose=0)\n",
        "\n",
        "print(\"Test Loss(MSE):\", test_loss)\n",
        "print(\"Test R-squared:\", test_r_squared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUEOgiyQmZha",
        "outputId": "2f99fe32-7d1b-4190-f10e-30da1ec6fa7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 128)               1664      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,882\n",
            "Trainable params: 12,722\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "NN.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk30IXTTBDLT"
      },
      "source": [
        "##### 새로운 값 예측 (기존 데이터 활용)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1ZWyGre42AS",
        "outputId": "0f0f06e1-5987-4663-e7a2-3d7c06c07785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 예측값\n",
            "Resistivity : 159.91487 \n",
            "Ex_resistivity : 149.91791\n"
          ]
        }
      ],
      "source": [
        "# 예측할 데이터 준비\n",
        "new_data = tf.constant(X_data.iloc[-1:])  # 예시 입력 데이터\n",
        "\n",
        "# 모델을 사용하여 예측 수행\n",
        "predictions = NN(new_data)\n",
        "\n",
        "# 예측 결과 출력\n",
        "print(\"Model 예측값\")\n",
        "print(\"Resistivity :\",predictions[0][0].numpy(),\"\\nEx_resistivity :\",predictions[0][1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLElweRW_NuA",
        "outputId": "dfbf48dd-deaa-4278-ca0e-deaca13d2b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "정답 값\n",
            "Resistivity : 165.5041494 \n",
            "Ex_resistivity : 156.0831481\n"
          ]
        }
      ],
      "source": [
        "#정답 값 출력\n",
        "print(\"정답 값\")\n",
        "print(\"Resistivity :\",Y_data.iloc[-1,0],\"\\nEx_resistivity :\",Y_data.iloc[-1,1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
